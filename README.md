# Machine Learning apprentissage

<a name="toc"/>

[Introduction](#introduction)

[1. Sujets d'√©tudes sur le Machine Learning](#sujet)

- [Section 1 : Rappel de probabilit√© et de Statistique](#section1)

- [Section 2 : L'apprentissage statistique](#section2)
- [Section 3 : L'incertitude dans le Machine Learning](#section3)
- [Section 4 : R√©gression lin√©aire](#section4)
- [Section 5 : Classification](#section5)
- [Section 6 : Les m√©thodes de r√©√©chantillonnage](#section6)
- [Section 7 : S√©lection de Mod√®le Lin√©aire](#section7)
- [Section 8 : Extensions au mod√®le lin√©aire](#section8)
- [Section 9 : M√©thodes bas√©es sur les arbres](#section9)
- [Section 10 : Deep Learning](#section10)
- [Section 11 : ...](#section11)
- [Section 12 : MachineLearnia Python pour le ML et le DL](#section12)
- [Section 13 : MachineLearnia Projet Python : Covid19](#section13)
- [Section 14 : ...](#section14)
- [Section .. : ...](#section15)
- [Section .. : ...](#section16)

[2. Laboratoire sur le Machine Learning](#lab)

- [Regression Lin√©aire](#lrl)
- [Neurone artificiel](#dlna)
- ...
- ...

[Appendice](#app)

------

<a name="introduction"/>

## Introduction

[Retour TOC](#toc)

Une premi√®re √©tape, avant de se lancer dans les algorithmes du Machine Learning, est l'apprentissage de certaines notions de probabilit√©s et de statistiques. Le Machine Learning utilise des algorithmes qui aident la machine √† apprendre. Par exemple, ces algorithmes servent √† reconna√Ætre des visages sur des images ou √† pr√©dire les pr√©f√©rences de choix de filmes des utilisateurs de streaming tel que Netflix ou YouTube. Ces pr√©dictions sont  bas√©es sur les caract√©ristiques (ou des variables ou des features) de comportement de visionnage des utilisateurs, elles peuvent √™tre le type de filme ou le temps pass√© √† les regarder.

Ces algorithme de Machine Learning sont indispensables pour identifier toutes les solutions bas√©es sur les nombreuses variables en entr√©es (pouvant √™tre de quelques milliers) car il serait tr√®s compliqu√© de traiter cette quantit√© de caract√©ristiques en programmation classique c'est √† dire que cela demanderait le d√©veloppement de millions (voir de milliards) de combinaisons en fonction du nombre de variables utilis√©es en source (il faudrait d√©velopper une condition sur chaque cas possible). 

[[9]( https://en.wikipedia.org/wiki/Transistor_count)] Avec la mont√©e en puissance de calcul des machines durant ces 40 derni√®res ann√©es, par exemple le nombre de transistor dans les microprocesseurs.

<img 
    style="display: block; 
           margin-left: auto;
           margin-right: auto;
           width: 50%;"
    src=".\images\Moore's_Law_Transistor_Count_1970-2020.png" 
    alt="Moore's law">
</img>

[[10]( https://www.i-scoop.eu/big-data-action-value-context/data-age-2025-datasphere/)] et l'augmentation du volume d'information disponible pour tout traitement, par exemple le Big Data. 

<img 
    style="display: block; 
           margin-left: auto;
           margin-right: auto;
           width: 40%;"
    src=".\images\Data_Evolution.png" 
    alt="Data_Evolution">
</img>

Les scientifiques ont compris qu'en utilisant, entre autres, la bo√Æte √† outils des probabilit√©s et la bo√Æte √† outils des statistiques qu'il devenait possible de donner √† une machine la capacit√© d'apprendre sans la programmer de fa√ßon explicite.

Voici une liste des probl√®mes les mieux adapt√©s pour les algorithme de Machine Learning

1.	Classer les nombres en nombres premiers et non premiers.
2.	D√©tecter une fraude potentielle dans les transactions par carte de cr√©dit.
3.	D√©terminer le temps qu'il faut √† un objet qui tombe pour toucher le sol.
4.	D√©terminer le cycle optimal des feux de signalisation dans un carrefour tr√®s fr√©quent√©e.

Avant d'aborder les probabilit√©s, il est important de pr√©ciser que la technique d'apprentissage la plus courante s'inspire de l'apprentissage supervis√©e, c'est √† dire que nous fournissons aux algorithmes des donn√©es d'apprentissages (Dataset) qui sont utilis√©es pour cr√©er des mod√®les. 
Par exemple, l'id√©e est de fournir √† un algorithme, un tableau de donn√©es contenant deux variables $X$ et $Y$ et, ensuite, cet algorithme doit d√©terminer la relation qui relie la variable $X$ √† $Y$, c'est √† dire $Y=f(X)$ + $\epsilon$ avec $\epsilon$  est un terme d'erreur al√©atoire et ind√©pendant de $X$ , c'est une erreur irr√©ductible qui peut contenir des variables non mesur√©es pour pr√©dire $Y$. 

C'est une des raisons principales de l'utilisation des probabilit√©s. **Quelle est le niveau d'incertitude de ma fonction (de mon mod√®le)? Quel mod√®le utiliser avec des informations incompl√®tes?** Les probabilit√©s nous donne les outils pour quantifier l'incertitude des √©v√©nements et pour raisonner de mani√®re sens√©e (math√©matique), c'est √† dire que la gestion de l'incertitude ne doit pas √™tre due √† la chance ou au hasard (attention, √† ne pas confondre avec le hasard des variables al√©atoire). **La probabilit√© quantifie la vraisemblance qu'un √©v√®nement va se produire et fournit les outils n√©cessaires pour g√©rer l'incertitude.**

 De mani√®re g√©n√©rale, **l'apprentissage statistique supervis√© consiste √† construire un mod√®le statistique pour pr√©dire ou estimer une sortie en fonction d'une ou plusieurs entr√©es**. 

A contrario, **avec l'apprentissage statistique non supervis√©, il y a des entr√©es mais pas de sortie supervis√©e**; n√©anmoins, nous pouvons apprendre des relations et des structures √† partir de ces donn√©es. 



------

<a name="sujet"/>

## 1. Sujets d'√©tudes sur le Machine Learning ##

[Retour TOC](#toc)

L'apprentissage du Machine Learning n√©cessite l'√©tude de plusieurs domaines. Ces domaines sont repris ci-dessous avec un lien sur le document explicatif.



------

<a name="section1"/>

### Section 1 : Rappel de probabilit√© et de Statistique ###

[Retour TOC](#toc)

------

**La th√©orie des probabilit√©s** est une branche des math√©matiques qui traitent des propri√©t√©s de certaines structures mod√©lisant des ph√©nom√®nes o√π le "hasard" intervient. 

Cette introduction √† la probabilit√© se trouve sur ce lien [Rappel de probabilit√© et de statistique](./docs/Rappel_Probabilite_et_Statistique.md) .

Un compl√©ment √† la premi√®re partie qui fait r√©f√©rence au cours de Samuel Leong [[18]](https://see.stanford.edu/materials/aimlcs229/cs229-prob.pdf) [Th√©orie des probabilit√©s Samuel Long](./docs/Probabilite_Samuel_Long.md) .

------

<a name="section2"/>

### Section 2 : L'apprentissage statistique [[6](https://www.statlearning.com/)] ###

[Retour TOC](#toc)

------

Par essence, l'apprentissage statistique fait r√©f√©rence √† un ensemble d'approches permettant d'estimer $f$. Dans ce chapitre, nous pr√©sentons certains des concepts th√©oriques cl√©s qui interviennent dans l'estimation de $f$ , ainsi que des outils permettant d'√©valuer les estimations obtenues.

Avec $f$ une certaine fonction Ô¨Åx√©e mais inconnue de $X_1,...,X_p$, et $\varepsilon$ est un terme d'erreur al√©atoire, qui est ind√©pendant de $X$ et a une moyenne de z√©ro. Dans cette formule, $f$ repr√©sente l'information syst√©matique que $X$ fournit sur $Y$ .

**Les variables d'entr√©e** sont g√©n√©ralement d√©sign√©es par le symbole $X$, avec un indice pour les distinguer. Ainsi, $X_1$ pourrait √™tre le budget de la t√©l√©vision, $X_2$ celui de la radio et $X_3$ celui des journaux. **Les entr√©es portent des noms** diÔ¨Ä√©rents, tels que **pr√©dicteurs**, **variables ind√©pendantes**, **caract√©ristiques (features)**, ou parfois juste variables.

**La variable de sortie** - dans ce cas, les ventes - est souvent **appel√©e la r√©ponse** ou **la variable d√©pendante**, et est g√©n√©ralement d√©sign√©e par le symbole $Y$ . 

Plus g√©n√©ralement, supposons que nous observions une r√©ponse quantitative $Y$ et $p$ pr√©dicteurs diÔ¨Ä√©rents, $X_1,X_2,..., X_p$. Nous supposons qu'il existe une relation entre $Y$ et $X=\ (X_1,X_2,...,X_p)$, qui peut s'√©crire sous la forme tr√®s g√©n√©rale suivante $\boxed{Y=f(X)+\ \epsilon}$.

La compr√©hension de l'utilisation de la probabilit√© et de la statistique commence par [L'apprentissage statistique](./docs/L_Apprentissage_Statistique.md) .

------

<a name="section3"/>

### Section 3 : L'incertitude dans le Machine Learning [[5]( https://machinelearningmastery.com/uncertainty-in-machine-learning/)]  ###

[Retour TOC](#toc)

------

Le Machine Learning appliqu√© n√©cessite de g√©rer  l'incertitude. Il existe de nombreuses sources d'incertitude dans un  projet de Machine Learning, notamment

- **la variance** des valeurs de donn√©es sp√©cifiques,
- **l'√©chantillon de donn√©es** collect√©es et
- **la nature imparfaite de tout mod√®le** d√©velopp√© √† partir de ces donn√©es.

**La gestion de l'incertitude inh√©rente √† le Machine Learning pour la mod√©lisation pr√©dictive peut √™tre r√©alis√©e gr√¢ce aux  outils et techniques de probabilit√©**, un domaine sp√©cifiquement con√ßu pour g√©rer l'incertitude : [L'incertitude dans le Machine Learning](./docs/L_incerrtitude_Machine_Learning.md)

------

<a name="section4"/>

### Section 4 : R√©gression lin√©aire [[6](https://www.statlearning.com/)]  [[19]](https://see.stanford.edu/Course/CS229) ###

[Retour TOC](#toc)

------

La r√©gression lin√©aire est un outil utile pour pr√©dire une r√©ponse quantitative. Bien qu'elle puisse sembler quelque peu ennuyeuse compar√©e √† certaines des approches d'apprentissage statistique plus modernes, la r√©gression lin√©aire reste une m√©thode d'apprentissage statistique utile et largement utilis√©e. On ne saurait trop insister sur l'importance de bien comprendre la r√©gression lin√©aire avant d'√©tudier des m√©thodes d'apprentissage plus complexes. 

Le d√©tail dans cette section [R√©gression Lin√©aire](./docs/Regression_Lineaire.md) 

Ce lien contient les notes [Lectures Notes R√©gression Lin√©aire](./docs/Regression_Lineaire_Stanford.md) du cours donn√© √† Stanford par Andrew Ng sur le Machine Learning  [[19]](https://see.stanford.edu/Course/CS229)

------

<a name="section5"/>

### Section 5 : Classification  [[6](https://www.statlearning.com/)] ###

[Retour TOC](#toc)

------

Le processus de classification permet de pr√©dire des r√©ponses qualitatives.

La pr√©diction d'une r√©ponse qualitative pour une observation peut √™tre qualifi√©e de classification de cette observation, car elle implique l'attribution de l'observation √† une cat√©gorie ou √† une classe. 

D'autre part, les m√©thodes utilis√©es pour la classification commencent souvent par pr√©dirent la probabilit√© que l'observation appartient √† chacune des cat√©gories d'une variable qualitative. En ce sens, elles se comportent √©galement comme des m√©thodes de r√©gression.

Dans cette section [Classification](./docs/Classification.md) , nous passons en revue

------

<a name="section6"/>

### Section 6 : Les m√©thodes de r√©√©chantillonnage  [[6](https://www.statlearning.com/)] ###

[Retour TOC](#toc)

------

Les m√©thodes de r√©√©chantillonnage sont un outil indispensable des statistiques modernes. Elles consistent √† tirer de mani√®re r√©p√©t√©e des √©chantillons d'un ensemble d'apprentissage et √† r√©ajuster un mod√®le d'int√©r√™t sur chaque √©chantillon afin d'obtenir des informations suppl√©mentaires sur le mod√®le ajust√©. Par exemple, afin d'estimer la variabilit√© de l'ajustement d'une r√©gression lin√©aire, nous pouvons tirer √† plusieurs reprises diff√©rents √©chantillons des donn√©es d'apprentissage, ajuster une r√©gression lin√©aire √† chaque nouvel √©chantillon, puis examiner dans quelle mesure les ajustements r√©sultants diff√®rent. Une telle approche peut nous permettre d'obtenir des informations qui ne seraient pas disponibles en ajustant le mod√®le une seule fois en utilisant l'√©chantillon d'entra√Ænement original.

Les approches de r√©√©chantillonnage peuvent √™tre co√ªteuses en termes de calcul, car elles impliquent l'ajustement de la m√™me m√©thode statistique plusieurs fois en utilisant diff√©rents sous-ensembles de donn√©es d'apprentissage. Cependant, gr√¢ce aux r√©cents progr√®s de la puissance de calcul, les exigences de calcul des m√©thodes de r√©√©chantillonnage ne sont g√©n√©ralement pas prohibitives. 

Nous abordons deux des m√©thodes de r√©√©chantillonnage les plus couramment utilis√©es, la validation crois√©e et le bootstrap. Ces deux m√©thodes sont des outils importants dans l'application pratique de nombreuses proc√©dures d'apprentissage statistique. Par exemple, la validation crois√©e peut √™tre utilis√©e pour estimer l'erreur de test associ√©e √† une m√©thode d'apprentissage statistique donn√©e afin d'√©valuer ses performances, ou pour s√©lectionner le niveau de flexibilit√© appropri√©. Le processus d'√©valuation de la performance d'un mod√®le est connu sous le nom d'√©valuation de mod√®le, tandis que le processus de s√©lection du niveau de flexibilit√© appropri√© pour un mod√®le est connu sous le nom de s√©lection de mod√®le. Le bootstrap est utilis√© dans plusieurs contextes, le plus souvent pour fournir une mesure de la pr√©cision de l'estimation d'un param√®tre ou d'une m√©thode d'apprentissage statistique donn√©e. Le d√©tail de cette section sur le lien [M√©thodes_de_R√©√©chantillonnage](./docs/Methodes_Reechantillonage.md)

------

<a name="section7"/>

### Section 7 : S√©lection de Mod√®le Lin√©aire  [[6](https://www.statlearning.com/)] ###

[Retour TOC](#toc)

------

Le mod√®le lin√©aire pr√©sente des avantages distincts en termes d'inf√©rence et, sur les probl√®mes du monde r√©el, il est souvent √©tonnamment comp√©titif par rapport aux m√©thodes non lin√©aires. Par cons√©quent, avant de passer au monde non lin√©aire, nous examinons dans cette section certaines fa√ßons d'am√©liorer le mod√®le lin√©aire simple, en rempla√ßant l'ajustement des moindres carr√©s par d'autres proc√©dures d'ajustement. Le d√©tail de cette section sur le lien [S√©lection_de_Mod√®le_Lin√©aire](./docs/Modele_Lineaire.md)

------

<a name="section8"/>

### Section 8 : Extensions au mod√®le lin√©aire  [[6](https://www.statlearning.com/)] ###

Jusqu'√† pr√©sent, nous nous sommes principalement concentr√©s sur les mod√®les lin√©aires. Les mod√®les lin√©aires sont relativement simples √† d√©crire et √† mettre en ≈ìuvre, et pr√©sentent des avantages par rapport √† d'autres approches en termes d'interpr√©tation et d'inf√©rence. 

Cependant, la r√©gression lin√©aire standard peut pr√©senter des limites importantes en termes de pouvoir pr√©dictif. Cela est d√ª au fait que l'hypoth√®se de lin√©arit√© est presque toujours une approximation, et parfois une mauvaise approximation. 

La section 7 montre que nous pouvons am√©liorer les moindres carr√©s en utilisant la r√©gression ridge, le lasso, la r√©gression en composantes principales et d'autres techniques. Dans ce contexte, l'am√©lioration est obtenue en r√©duisant la complexit√© du mod√®le lin√©aire, et donc la variance des estimations. 

Mais nous utilisons toujours un mod√®le lin√©aire, qui ne peut √™tre am√©lior√© que jusqu'√† un certain point ! Dans cette section, nous assouplissons l'hypoth√®se de lin√©arit√© tout en essayant de maintenir autant d'interpr√©tabilit√© que possible. 

Pour ce faire, nous examinons des extensions tr√®s simples des mod√®les lin√©aires, comme la r√©gression polynomiale et les fonctions √©chelon, ainsi que des approches plus sophistiqu√©es telles que les splines, la r√©gression locale et les mod√®les additifs g√©n√©ralis√©s.

Le d√©tail de cette section sur le lien [Extensions au mod√®le lin√©aire](./docs/Extensions_au_mod√®le_lin√©arit√©.md)

[Retour TOC](#toc)

------

<a name="section9"/>

### Section 9 : M√©thodes bas√©es sur les arbres  [[6](https://www.statlearning.com/)] ###

[Retour TOC](#toc)

Dans cette section, nous d√©crivons les m√©thodes de r√©gression et de classification bas√©es sur les arbres. Ces m√©thodes consistent √† stratifier ou √† segmenter l'espace de pr√©diction en un certain nombre de r√©gions simples. Afin d'effectuer une pr√©diction pour une observation donn√©e, nous utilisons g√©n√©ralement la valeur de r√©ponse moyenne ou modale pour les observations d'apprentissage dans la r√©gion √† laquelle elle appartient. 

Puisque l'ensemble des r√®gles de division utilis√©es pour segmenter l'espace pr√©dicteur peut √™tre r√©sum√© dans un arbre, ces types d'approches sont connus comme des ***m√©thodes d'arbre de d√©cision***.

Les m√©thodes bas√©es sur les arbres sont simples et utiles pour l'interpr√©tation. Cependant, elles ne sont g√©n√©ralement pas comp√©titives avec les meilleures approches d'apprentissage supervis√©, telles que celles pr√©sent√©es aux sections 7 et 8, en termes de pr√©cision de pr√©diction. C'est pourquoi, dans cette section, nous pr√©sentons √©galement les arbres de type ***bagging***, ***random forests***, ***boosting*** et ***r√©gression additive bay√©sienne***. Chacune de ces approches implique la production de plusieurs arbres qui sont ensuite combin√©s pour produire une seule pr√©diction consensuelle. Nous verrons que la combinaison d'un grand nombre d'arbres permet souvent d'am√©liorer consid√©rablement la pr√©cision des pr√©dictions, au prix d'une certaine perte d'interpr√©tation.

Le d√©tail de cette section sur le lien [M√©thodes bas√©es sur les arbres](./docs/Methodes_basees_sur_les_arbres.md)

------

<a name="section10"/>

### Section 10 : Deep Learning  [[6](https://www.statlearning.com/)] [[4](https://machinelearnia.com/)] ###

Cette section aborde le sujet important de l'apprentissage profond. Au moment de la r√©daction de ce document (2020), l'apprentissage profond est un domaine de recherche tr√®s actif dans les communaut√©s de l'apprentissage automatique et de l'intelligence artificielle. La pierre angulaire de l'apprentissage profond est le r√©seau neuronal.

Le d√©tail de cette section sur le lien [Deep Learning](./docs/Deep_Learning.md)

Un autre document contenant une m√©thode simple d' apprentissage du Deep Learning divis√© en plusieurs le√ßons : [Le√ßons_Deep_Learning](./docs/Deep_Learning_ABC.md)

[Retour TOC](#toc)

------

<a name="section11"/>

### Section 11 : ... ###

[Retour TOC](#toc)

------

<a name="section12"/>

### Section 12 : Python pour le Machine Learning et le Deep Learning [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

1. **Numpy :**

  Numpy (Numerical Python) est une biblioth√®que essentielle pour la programmation scientifique en Python. Avec ses fonctions puissantes de manipulation de tableaux multidimensionnels et ses outils de calcul math√©matiques de haut niveau, elle sert de pilier pour de nombreuses autres biblioth√®ques dans l'√©cosyst√®me Python li√© √† l'analyse de donn√©es, au Machine Learning (ML) et au Deep Learning (DL). Voici quelques raisons pour lesquelles l'apprentissage de Numpy est essentiel dans ces domaines :

  1. **Efficient de la manipulation des donn√©es** : Numpy offre une manipulation rapide et efficace des tableaux de donn√©es de toutes dimensions. C'est essentiel dans le ML et DL, o√π nous traitons souvent des ensembles de donn√©es volumineux.
  2. **Compatibilit√© avec d'autres biblioth√®ques** : Numpy s'int√®gre parfaitement avec d'autres biblioth√®ques importantes comme Pandas pour la manipulation de donn√©es, Matplotlib pour la visualisation, et Scikit-learn pour le ML. En DL, des frameworks tels que TensorFlow et PyTorch utilisent √©galement des structures de donn√©es similaires √† celles de Numpy.
  3. **Calcul math√©matique** : Le ML et DL impliquent beaucoup de calculs math√©matiques, notamment l'alg√®bre lin√©aire, le calcul statistique, et les transformations de Fourier. Numpy offre une grande vari√©t√© de fonctions int√©gr√©es pour r√©aliser ces calculs de mani√®re optimis√©e.
  4. **Vectorisation** : Une caract√©ristique importante de Numpy est la vectorisation qui permet d'effectuer des op√©rations sur des tableaux entiers sans avoir √† √©crire des boucles. Cela rend les calculs beaucoup plus rapides et am√©liore l'efficacit√© du code, ce qui est particuli√®rement important pour le traitement de grands volumes de donn√©es en ML et DL.
  5. **Transparence et contr√¥le** : Contrairement √† certaines biblioth√®ques de ML et DL qui cachent les d√©tails de mise en ≈ìuvre, l'utilisation de Numpy donne plus de contr√¥le et de transparence sur la fa√ßon dont les op√©rations sont effectu√©es. Cela peut √™tre utile pour la personnalisation, le d√©bogage et l'am√©lioration de la performance des mod√®les.

  En r√©sum√©, apprendre Numpy n'est pas seulement b√©n√©fique, mais essentiel pour toute personne travaillant dans le ML ou le DL. C'est une comp√©tence fondamentale qui vous permettra de manipuler des donn√©es, d'impl√©menter des algorithmes, d'int√©grer votre travail avec d'autres biblioth√®ques, et finalement de cr√©er des solutions efficaces pour des probl√®mes de ML et DL complexes.

Le document reprenant tous ces points se trouve en cliquant sur le lien [Python_pour_le_machine_learning_Numpy](./docs/Python_pour_la_machine_learning_Numpy.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python_pour_le_machine_learning_Numpy](./codes/Python_pour_la_machine_learning_Numpy.ipynb)

2. **Matplotlib** :

  [Retour TOC](#toc)

  Permettez une petite question : Pourquoi cr√©e-t-on des graphiques dans la vie ? 

  C'est pour visualiser les choses sur lesquelles on travaille, qu'il s'agisse de donn√©es ou d'un mod√®le. C'est pour mieux comprendre le probl√®me sur lequel on travaille. En d'autres termes, un graphique est cens√© aider √† la r√©solution de probl√®mes. 

  Pourtant, pour nombre de personnes qui utilisent Matplotlib, c'est l'inverse qui se produit.Beaucoup de personnes vont cr√©er un graphique et dans ce graphique, il y aura des erreurs. Ainsi, au lieu d'aider √† r√©soudre leurs probl√®mes, ce graphique leur donne un nouveau probl√®me qu'ils doivent d'abord r√©soudre avant de s'attaquer √† leurs vrais probl√®mes. Il suffit de consulter le premier forum tel que Stack Overflow pour voir le nombre de personnes qui ont des probl√®mes avec Matplotlib.

  Pourtant, Matplotlib est tr√®s simple √† utiliser et en principe, aucun bug ne devrait survenir avec ce package. 

  Si les gens rencontrent parfois des probl√®mes, c'est d'une part parce qu'ils essaient d'ajouter beaucoup trop de d√©tails √† leur courbe. Ils perdent du temps √† perfectionner leur graphique alors qu'ils devraient se concentrer sur leur probl√®me de machine learning. D'autre part, c'est parce qu'il existe deux m√©thodes pour cr√©er des graphiques dans Matplotlib.

  Une m√©thode est orient√©e objet et l'autre est plus basique. Comme l'indique Matplotlib sur leur site officiel, les gens ont tendance √† m√©langer ces deux m√©thodes, et ils ne devraient pas. 

  Il sera expliqu√© comment cr√©er des graphiques qui ne soient ni trop simples, ni trop sophistiqu√©s. Juste les graphiques parfaits qu'il faut, sans jamais avoir de bug dans ces graphiques. C'est vraiment tr√®s simple.

  Comme mentionn√© pr√©c√©demment, il existe deux m√©thodes pour cr√©er des graphiques avec Matplotlib. La m√©thode la plus simple est d'utiliser une fonction appel√©e plot qui provient du module Pyplot. 

Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning MatPlotLib](./docs/Python_pour_la_machine_learning_MatPlotLib.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning MatPlotLib](./codes/Python_pour_la_machine_learning_MatPlotLib.ipynb)

3. **SciPy**

  [Retour TOC](#toc)

  Nous allons voir comment faire du calcul scientifique avec $ùëÜùëêùëñùëÉùë¶$. √Ä l'int√©rieur de ce package, on retrouve des outils absolument  incroyables pour faire du math√©matique, et bizarrement, beaucoup de  data scientists oublient de les utiliser.

  En l'occurrence, on va voir comment :

  - Faire des interpolations
  - S'attaquer √† l'optimisation de probl√®me
  - Proc√©der au traitement du signal, ce qui inclura la Transform√©e de Fourier, extr√™mement puissante pour filtrer des signaux.

Nous terminerons en voyant comment faire du traitement d'image avec $ùëõùëëùëñùëöùëéùëîùëí$. Je t√©l√©chargerai m√™me en live une image qui nous vient d'internet pour  que nous puissions faire l'analyse avec diff√©rentes techniques et en  retirer des informations int√©ressantes dans un tableau $ùëõùë¢ùëöùëùùë¶$.

  Alors, quand on consulte la documentation officielle de $ùëÜùëêùëñùëÉùë¶$, qui est disponible √† cette adresse : https://docs.scipy.org/doc/scipy/reference/index.html, on peut se rendre compte que dans $SciPy$, on a tout un tas de petits  modules qui nous permettent de r√©aliser des actions scientifiques bien  pr√©cises.  Par exemple, on va retrouver un module pour faire de l'alg√®bre  lin√©aire ou un autre pour faire des statistiques. En fait, c'est un peu  comme dans $ùëõùë¢ùëöùëùùë¶$ o√π nous avions aussi ùëôùëñùëõùëéùëôùëî et ùë†ùë°ùëéùë°ùë†.

On va s'int√©resser tout de suite au module $ùëñùëõùë°ùëíùëüùëùùëúùëôùëéùë°ùëí$ et $ùëõùëëùëñùëöùëéùëîùëí$.

Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning Scipy](./docs/Python_pour_la_machine_learning_Scipy.md)

Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning Scipy](./codes/Python_pour_la_machine_learning_Scipy.ipynb)

4. **Panda, las bases :  Analyse du Titanic**

  [Retour TOC](#toc)

  Est-ce que vous saviez que vous aviez plus de chances de survivre √† bord du Titanic si vous √©tiez un homme voyageant en troisi√®me classe plut√¥t qu'un homme voyageant en seconde classe ?

  Dans cette le√ßon de la s√©rie Python sp√©cial "machine learning", vous allez apprendre √† utiliser pandas, qui est l'outil le plus important √† conna√Ætre quand on souhaite travailler avec des donn√©es. 

  Si j'ai mentionn√© une telle chose, c'est parce qu'avec pandas, vous pouvez r√©aliser tout ce que vous pourriez imaginer avec des donn√©es. 

  Vous pouvez charger vos propres donn√©es dans Python, puis les manipuler, les nettoyer, les observer et les analyser. 

  Vous pouvez prendre deux datasets et les assembler ensemble. Bref, vous pouvez faire tout ce genre de choses, et tout cela gr√¢ce √† une structure tr√®s simple √† comprendre : le DataFrame.

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning Pandas Analyse du Titanic](./docs/Python_pour_la_machine_learning_Pandas_Analyse_du_Titanic.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning Pandas Analyse du Titanic](./codes/Python_pour_la_machine_learning_Pandas_Anamyse_du_Titanic.ipynb)

5. **Panda, Time series**

  [Retour TOC](#toc)

  Nous avons ici l'une des techniques de trading les plus populaires. 

<img 
    style="display: block; 
           margin-left: auto;
           margin-right: auto;
           width: 50%;"
    src=".\images\Python_Pandas_Fig_000016.png" 
    alt="Python_Pandas_Fig">
</img>

- Pourtant, je vais vous expliquer pourquoi vous ne devez jamais l'utiliser sur du Bitcoin, au risque de perdre tout votre argent.Nous allons voir comment utiliser $Pandas$ pour travailler sur des probl√®mes de $time\ series$. Cela va typiquement inclure l'√©tude du climat, l'analyse de la bourse ou tout autre ph√©nom√®ne qui √©volue avec le temps. 

  En r√©alit√©, Pandas a m√™me √©t√© sp√©cifiquement d√©velopp√© pour aborder ce type de probl√®me, donc nous y trouverons une multitude de fonctionnalit√©s pour travailler sur des time series.

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning Pandas Time Series](./docs/Python_pour_la_machine_learning_Pandas_Time_Series.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour la machine learning Pandas Time Series](./codes/Python_pour_la_machine_learning_Pandas_Time_Series.ipynb)

6. **Seaborn**

   [Retour TOC](#toc)

   Vous savez, quand on cherche √† r√©soudre un probl√®me, il faut bien  souvent commencer par visualiser ce probl√®me. Par exemple, en physique,  on cherche √† observer des ph√©nom√®nes, et en "data science", on cherche √† visualiser nos donn√©es. 

   Et comme vous le savez, pour visualiser des donn√©es dans Python, il  existe Matplotlib. Mais bon, Matplotlib, eh bien, c'est Matplotlib quoi. 

   D√©j√†, c'est peu esth√©tique, car √ßa ressemble √† √ßa,  

![Python_Seaborn_Fig](./images/Python_Seaborn_Fig_000001.png)

- alors qu'en r√©alit√©, c'est superbe. 

  Et quand vous vous d√©brouillez pour sortir un graphique √† peu pr√®s  sympathique, il faut √©crire une tonne de code, et nous, on n'est pas l√†  pour √ßa. 

  Notre job, c'est de r√©soudre des probl√®mes. C'est pour cette raison  qu'il existe Seaborn, construit sur la base de Matplotlib et de Pandas,  et qui permet de r√©aliser une visualisation de donn√©es tr√®s pouss√©e en  √©crivant seulement une ligne de code. 

  Je r√©p√®te, avec Seaborn, vous pouvez cr√©er ce genre de graphiques en √©crivant simplement une seule ligne de code.  

![Python_Seaborn_Fig](./images/Python_Seaborn_Fig_000002.png)

Le document reprenant tous ces points se trouve en cliquant sur le lien  [Python pour le machine learning SEABORN](./docs/Python_pour_la_machine_learning_SEABORN.md)

Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SEABORN](./codes/Python_pour_la_machine_learning_SEABORN.ipynb)

7. **SKLEARN**

   [Retour TOC](#toc)

- **KNN, LinearRegression et SUPERVISED LEARNING**

  Est-ce que vous pensez que vous auriez surv√©cu au naufrage du Titanic ? 

  Nous allons d√©velopper un mod√®le de machine learning pour pr√©dire  quelles √©taient vos chances de survie, en prenant en compte votre √¢ge,  votre sexe, et la classe dans laquelle vous auriez voyag√©. 

  Nous allons utiliser Sklearn pour faire de l'apprentissage supervis√©  c'est √† dire comment estimer le prix d'un appartement, pr√©dire le cours  de la bourse, d√©tecter un objet sur une photo, ou m√™me calculer vos  chances de survie lors d'une catastrophe telle que celle du Titanic. 

  Mais avant tout, voyons bri√®vement ce qu'est le machine learning

  

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning SKLEARN KNN LinearRegression SUPERVISED LEARNING](./docs/Python_pour_la_machine_learning_SKLEARN_KNN_LinearRegression_SUPERVISED_LEARNING.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN KNN LinearRegression SUPERVISED LEARNING](./codes/Python_pour_la_machine_learning_SKLEARN_KNN_LinearRegression_SUPERVISED_LEARNING.ipynb)

- **Train_test_split, Cross Validation, GridSearchCV**

  [Retour TOC](#toc)

  Je vais vous d√©voiler les techniques pour entra√Æner un mod√®le, l'optimiser et l'√©valuer avec la bonne m√©thodologie. 

  Nous d√©couvrirons comment cr√©er un Trainset et un Testset √† l'aide de la fonction $train\_test\_split()$. 

  Ensuite, nous aborderons la validation d'un mod√®le gr√¢ce √† la technique de $cross-validation$. 

  Enfin, nous explorerons comment am√©liorer un mod√®le en utilisant $GridSearchCV$ et les courbes d'apprentissage.

  

  Le document reprenant tous ces points se trouve en cliquant sur le lien  [Python pour le machine learning SKLEARN Train test split Cross Validation GridSearchCV](./docs/Python_pour_la_machine_learning_SKLEARN_Train_test_split_Cross_Validation_GridSearchCV.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN Train test split Cross Validation GridSearchCV](./codes/Python_pour_la_machine_learning_SKLEARN_Train_test_split_Cross_Validation_GridSearchCV.ipynb)

- **Metrics Regression**

  [Retour TOC](#toc)

  Nous allons parler de m√©triques, et plus pr√©cis√©ment de m√©triques de  r√©gression. En effet, beaucoup d'entre vous se demandent quelle est la  diff√©rence entre la RMSE, la MAE, le coefficient R carr√©, et dans  quelles situations utiliser l'un plut√¥t que l'autre. 

  

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning SKLEARN Metrics Regression](./docs/Python_pour_la_machine_learning_SKLEARN_Metrics_Regression.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN Metrics Regression](./codes/Python_pour_la_machine_learning_SKLEARN_Metrics_Regression.ipynb)

- **Make Scorer**

  [Retour TOC](#toc)

  Je vais vous montrer comment utiliser la fonction make_scorer qui  nous vient de Scikit-Learn. Cette fonction est extr√™mement utile car  elle vous permet de d√©velopper vos propres m√©triques pour les utiliser  dans des algorithmes de cross-validation ou des algorithmes comme  GridSearchCV. Croyez-moi, d√©velopper ses propres m√©triques pour √©valuer  son mod√®le de machine learning est quelque chose qui arrive tr√®s souvent dans le monde professionnel. 

  En effet, quand vous travaillez avec un client, surtout dans les  secteurs industriels, il arrive tr√®s souvent que votre client se fiche  un peu de votre coefficient de d√©termination ou de votre erreur  quadratique moyenne. Lui, il vous fournit un projet avec un cahier des  charges, dans lequel il y a des contraintes que vous devez respecter. Et parmi ces contraintes, on va trouver des mesures de performances qui  sont sp√©cifiques au projet sur lequel vous travaillez.

  

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning SKLEARN Make Scorer](./docs/Python_pour_la_machine_learning_SKLEARN_Make_Scorer.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN Make Scorer](./codes/Python_pour_la_machine_learning_SKLEARN_Make_Scorer.ipynb)

- **PRE-PROCESSING + PIPELINE**

  [Retour TOC](#toc)

  Le data processing est l'une des √©tapes les plus importantes pour d√©velopper des mod√®les avec de bonnes performances. 

  Nous allons commencer par voir ce qu'est le data processing. Je vous montrerai les diff√©rentes techniques √† conna√Ætre. 

  Ensuite, nous verrons comment les mettre en ≈ìuvre avec Scikit-learn  et comment construire une cha√Æne de transformation avec la classe  Pipeline de Scikit-learn.

  

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning SKLEARN PRE PROCESSING PIPELINE](./docs/Python_pour_la_machine_learning_SKLEARN_PRE_PROCESSING_PIPELINE.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN PRE PROCESSING PIPELINE](./codes/Python_pour_la_machine_learning_SKLEARN_PRE_PROCESSING_PIPELINE.ipynb)

- **Feature Selection**

  [Retour TOC](#toc)

  Le document reprenant tous ces points se trouve en cliquant sur le lien [Python pour le machine learning SKLEARN Feature Selection](./docs/Python_pour_la_machine_learning_SKLEARN_Feature_Selection.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour le machine learning SKLEARN Feature Selection](./codes/Python_pour_la_machine_learning_SKLEARN_Feature_Selection.ipynb)

- **Apprentissage non supervis√©**

  [Retour TOC](#toc)

  Je vais vous pr√©senter les bases de l'apprentissage non supervis√©, la deuxi√®me branche tr√®s connue du machine learning et du deep learning.  Nous allons explorer les trois applications les plus importantes : 

  - le cloud storage, 
  - la d√©tection d'anomalies, et 
  - la r√©duction de dimension. 

  Le document reprenant tous ces points se trouve en cliquant sur le lien  [Python pour le machine learning SKLEARN Apprentissage Non Suppervis√©](./docs/Python_pour_la_machine_learning_SKLEARN_Apprentissage_Non_Suppervis√©.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour la machine learning SKLEARN Apprentissage Non Suppervis√©](./codes/Python_pour_la_machine_learning_SKLEARN_Apprentissage_Non_Suppervis√©.ipynb)

- **Ensemble BAGGING, BOOSTING et STACKING**
  [Retour TOC](#toc)

  Nous allons parler d'ensemble learning, une technique qui consiste √† entra√Æner plusieurs mod√®les de machine learning pour ensuite consid√©rer l'ensemble de leurs pr√©dictions. 

  Pour cela, il existe trois grandes m√©thodes : le bagging, le boosting et le stacking. 

  Les algorithmes qui reposent sur ces m√©thodes, comme l'algorithme de random forest, comptent parmi les plus performants dans le monde du machine learning.

  Le document reprenant tous ces points se trouve en cliquant sur le lien  [Python pour la machine learning Ensemble BAGGING BOOSTING STACKING](./docs/Python_pour_la_machine_learning_Ensemble_BAGGING_BOOSTING_STACKING.md)

  Le code notebook contenant tous ces points se trouve en cliquant sur ce lien [Python pour la machine learning Ensemble BAGGING BOOSTING STACKING](./Python_pour_la_machine_learning_Ensemble_BAGGING_BOOSTING_STACKING.ipynb)

------

<a name="section13"/>

### Section 13 : Projet Python: COVID19 [[4](https://machinelearnia.com/)] ###

Je vous propose de pratiquer tout ce que nous avons vu en travaillant  sur un vrai dataset. 

Je vous ai donc trouv√© un dataset tr√®s int√©ressant  qui va vous permettre de pratiquer tout ce que nous avons vu avec Pandas et Scikit-learn. Il s'agit du dataset "Diagnosis of COVID-19 and  Clinical Spectrum" qui est disponible sur Google et qui regroupe les  r√©sultats cliniques de plus de 5000 personnes, indiquant √† chaque fois  si la personne souffre de la maladie du COVID-19 ou non.

En g√©n√©ral, le travail de data scientist est divis√© en trois activit√©s. 

1. La premi√®re, c'est l'analyse et l'exploration de nos donn√©es, ce  qu'on appelle en anglais "exploratory data analysis". Ici, le but est de se familiariser avec le dataset et de comprendre les diff√©rentes  variables pour ensuite d√©finir une strat√©gie de mod√©lisation. 

   Le notebook traitant de cette partie se trouve sur ce lien [Python pour le machine learning Projet Coronavirus and Exploratory Data Analysis](./codes/Python_pour_la_machine_learning_Projet_Coronavirus_and_Exploratory_Data_Analysis.ipynb)

2. Une fois cette strat√©gie d√©finie, on passe √† la deuxi√®me activit√© :  le preprocessing. C'est ici que l'on transforme le dataset pour qu'il  soit pr√™t pour le d√©veloppement de mod√®les de machine learning. On va  encoder les donn√©es, √©liminer les valeurs manquantes, s√©lectionner des  variables, etc.

   Le notebook traitant de cette partie se trouve sur ce lien [Python pour le machine learning Projet Coronavirus and PRE-Traitement](./codes/Python_pour_la_machine_learning_Projet_Coronavirus_and_PRE-Traitement.ipynb)

3. Finalement, on arrive √† la troisi√®me activit√© : la mod√©lisation. Le  but est clair : cr√©er, entra√Æner, √©valuer et am√©liorer un mod√®le de  machine learning. On va peut-√™tre aussi comparer ce mod√®le avec d'autres mod√®les pour atteindre l'objectif initial.

   Le notebook traitant de cette partie se trouve sur ce lien [Python pour le machine learning Projet Coronavirus Mod√®le](./codes/Python_pour_la_machine_learning_Projet_Coronavirus_Mod√®le.ipynb)

[Retour TOC](#toc)

------

<a name="section14"/>

### Section 14 : ... ###

[Retour TOC](#toc)

------



<a name="section15"/>

### Section .. : ... ###

[Retour TOC](#toc)

------



<a name="section16"/>

### Section .. : ... ###

[Retour TOC](#toc)

------

<a name="lab"/>

## 2. Laboratoire sur le Machine Learning ##

[Retour TOC](#toc)

<a name="lrl"/>

### Labs 1 : Regression Lin√©aire [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

La recette de la r√©gression lin√©aire :

1. R√©colter des donn√©es
2. Donner √† la machine un mod√®le lin√©aire
3. Cr√©er la Fonction Co√ªt
4. Calculer le gradient et utiliser l‚Äôalgorithme de Gradient Descent avec  le Learning Rate qui prend le nom **d‚Äôhyperparam√®tre** de par son influence sur la performance finale du mod√®le (s‚Äôil est trop grand o√π trop petit, la fonction le Gradient Descent ne converge pas).

Un explication sur la Fonction co√ªt, ainsi qu'un exemple concret sur la Regression Lin√©aire se trouve sur ce lien [Labs Regression Lin√©aire](./labs/Labs_Regression_Lineaire.md) .

<a name="dlna"/>

### Labs 2 : Deep Learning : Programmation d'un neurone artificiel [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

Nous allons d√©velopper notre premier programme de neurone artificiel. Et pour √ßa nous allons impl√©menter toutes les √©quations que l'on a vu dans les derni√®res le√ßons.

Alors, pour d√©velopper notre programme de neurones artificiels, nous allons partir d'un Dataset $(X, y)$ de 100 lignes et de deux colonnes. 

Si on veut, on peut imaginer que ce Dataset repr√©sente des plantes avec la longueur et la largeur de leurs feuilles. Et notre but, c'est d'entra√Æner un neurone artificiel pour reconna√Ætre les plantes toxiques des plantes non toxique gr√¢ce √† ces donn√©es de r√©f√©rence.

Voici l'impl√©mentation. Le code se trouve sur [Labs_2_Deep_Learning_Programmation_neurone_artificiel](./codes/Labs_2_Deep_Learning_Programmation_neurone_artificiel.ipynb)

Une impl√©mentation objet se trouve sur ce lien : [Labs_2_Deep_Learning_Programmation_neurone_artificiel_OO](./codes/Labs_2_Deep_Learning_Programmation_neurone_artificiel_OO.ipynb)

### Labs 3 : Deep Learning : Programmation Chien vs Chat [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

Nous allons d√©velopper un programme de vision par ordinateur pour reconna√Ætre une photo de chat ou de chien. Donc ce qu‚Äôon aimerait faire, √ßa serait de fournir des photos de chats et de chiens √† notre code pour qu‚Äôils nous retourne un mod√®le qui soit capable de classer ce genre de photos.

Voici l'impl√©mentation. Le code se trouve sur [Labs_3_Deep_Learning_Chient_vs_Chat](./codes/Labs_3_Deep_Learning_Chient_vs_Chat.ipynb)

### Labs 4 : Deep Learning : Programmer un r√©seau de neurones √† 2 couches [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

Nous allons d√©velopper un r√©seau de neurones √† 2 couches.

Voici l'impl√©mentation. Le code se trouve sur [Labs_4_Deep_Learning_Programmation_neurone_artificiel_2_couches](./codes/Labs_4_Deep_Learning_Programmation_neurone_artificiel_2_couches.ipynb)

### Labs 5 : Deep Learning : R√©seau de neurones profond [[4](https://machinelearnia.com/)] ###

[Retour TOC](#toc)

Pour d√©velopper un r√©seau de neurones profonds, avec autant de couches que l'on d√©sire √† l'int√©rieur, nous allons repartir des √©quations qui nous avait permis de cr√©er un r√©seau de neurones √† deux couches. 

Voici l'impl√©mentation. Le code se trouve sur [Labs_5_Reseau_de_neurones_profonds](./codes/Labs_5_Reseau_de_neurones_profonds.ipynb)

------

<a name="app"/>

## Appendice ##

[Retour TOC](#toc)

-A- Notation et alg√®bre matricielle simple

-B- Calcul des d√©riv√©es partielles : descente de gradient

-C- Alg√®bre Lin√©aire

Lien vers [Appendice Math√©matique](./docs/Appendice_Mathematique.md)
