# [Appendice](#appendice)  #

[Retour README](../README.md)

<a name="toc"/>

[-A- Notation et alg√®bre matricielle simple](#a)

[-B- Calcul des d√©riv√©es partielles : descente de gradient](#b)

[-C- Alg√®bre Lin√©aire](#c)

- [C.1 Concepts et notations de base](#c-1)
  - [C.1.1 Notation de base](#c-1-1)

- [C.2 Multiplication matricielle](#c-2)

  - [C.2.1 Produits vectoriels](#c-2-1)

  - [C.2.2 Produits matrice-vecteur](#c-2-2)

  - [C.2.3 Produits matriciels](#c-2-3)

- [C.3 Op√©rations et propri√©t√©s](#c-3)

  - [C.3.1 Matrice d'identit√© et matrices diagonales](#c-3-1)

  - [C.3.2 La transposition](#c-3-2)

  - [C.3.3 Matrices sym√©triques](#c-3-3)

  - [C.3.4 La trace](#c-3-4)

  - [C.3.5 Normes](#c-3-5)

  - [C.3.6 Ind√©pendance lin√©aire et rang](#c-3-6)

  - [C.3.7 L'inverse](#c-3-7)

  - [C.3.8 Matrices orthogonales](#c-3-8)

  - [C.3.9 Plage et nulspace d'une matrice](#c-3-9)

  - [C.3.10 Le d√©terminant](#c-3-10)

  - [C.3.11 Formes quadratiques et matrices semi-d√©finies positives](#c-3-11)

  - [C.3.12 Valeurs propres et vecteurs propres](#c-3-12)

  - [C.3.13 Valeurs propres et vecteurs propres des matrices sym√©triques](#c-3-13)

- [C.4 Calcul matriciel](#c-4)

  - [C.4.1 Le gradient](#c-4-1)

  - [C.4.2 Le hessien](#c-4-2)

  - [C.4.3 Gradients et hessian des fonctions quadratiques et lin√©aires](#c-4-3)

  - [C.4.4 Les moindres carr√©s](#c-4-4)

  - [C.4.5 Gradients du d√©terminant](#c-4-5)

  - [C.4.6 Valeurs propres en tant qu'optimisation](#c-4-6)

<a name="A"/>

## [-A- Notation et alg√®bre matricielle simple](#a) ##

[Retour TOC](#toc)

[[6](https://www.statlearning.com/)] Le choix de la notation pour un manuel est toujours une t√¢che difficile. Nous utiliserons $n$ pour repr√©senter le nombre de points de donn√©es distincts, ou d'observations, dans notre √©chantillon, et nous utiliserons $p$ pour d√©signer le nombre de variables qui sont disponibles pour √™tre utilis√©es dans les pr√©dictions. 

Par exemple, l'ensemble de donn√©es sur les salaires se compose de 11 variables pour 3000 personnes. Nous avons donc $n=3000$ observations et $p=11$ variables (telles que l'ann√©e, l'√¢ge, la race, etc.). 

Dans certains exemples, $p$ peut √™tre tr√®s grand, de l'ordre de milliers ou m√™me de millions ; cette situation se pr√©sente assez souvent, par exemple, dans l'analyse de donn√©es biologiques modernes ou de donn√©es publicitaires sur Internet.

En g√©n√©ral, nous utilisons $x_{ij}$ comme la valeur de la $j^{√®me}$ variable pour la $i^{√®me}$ observation, avec $i=1,\ 2,...,n$ et $j=1,\ 2,...,p$. 
La variable $i$ est utilis√© pour indexer les √©chantillons ou les observations (de $1\ √†\ n$) et $j$ est utilis√© pour indexer les variables (de $1\ √†\ p$). $X$ d√©note une matrice $n \times p$ dont le $i,j^{√®me}$ √©l√©ment est $x_{ij}$. 
C'est-√†-dire,       

$$
X = 
 \begin{pmatrix}
  x_{11} & x_{12} & \cdots & x_{1p} \\
  x_{21} & x_{22} & \cdots & x_{2p} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{n1} & x_{n2} & \cdots & x_{np} 
 \end{pmatrix}
$$

Pour les lecteurs qui ne sont pas familiaris√©s avec les matrices, il est utile de visualiser $X$ comme une feuille de calcul contenant nombres avec $n$ lignes et $p$ colonnes.

Parfois, nous sommes int√©ress√©s par les rangs de $X$, que nous √©crivons comme $x_1,x_2,\ldots,x_n$. Ici $x_i$ est un vecteur de longueur $p$, contenant les $p$ mesures de variables pour la $i^{√®me}$  observation. 
C'est-√†-dire que

$$
X = 
 \begin{pmatrix}
  x_{i1}\\
  x_{i1}\\
  \vdots \\
  x_{ip} 
 \end{pmatrix}
$$

Les vecteurs sont repr√©sent√©s par d√©faut sous forme de colonnes. 
Par exemple, pour les donn√©es relatives aux salaires, $x_i$ est un vecteur de longueur 11, compos√© de l'ann√©e, de l'√¢ge, de la race et d'autres valeurs pour le $i^{√®me}$ individu.  A d'autres moments, nous sommes plut√¥t int√©ress√©s par les colonnes de $X$, que nous √©crivons comme $x_1,\ x_2,...,\ x_p$. 
Chacune des colonnes est un vecteur de longueur $n$. C'est-√†-dire,

$$
X = 
 \begin{pmatrix}
  x_{1j}\\
  x_{2j}\\
  \vdots \\
  x_{nj} 
 \end{pmatrix}
$$

Par exemple, pour les donn√©es sur les salaires, x_1 contient n=3000 valeurs par an. En utilisant cette notation, la matrice $X$ peut √™tre √©crite comme des valeurs pour l'ann√©e.

$$
X = 
 \begin{pmatrix}
  x_{1} \ x_{2} \ ... \ x_p\\
 \end{pmatrix},
$$

Ou

$$
X = 
 \begin{pmatrix}
  x_{1}^T\\
  x_{2}^T\\
  \vdots \\
  x_{n}^T 
 \end{pmatrix}
$$

La notation $^T$ d√©signe la transposition d'une matrice ou d'un vecteur. 
Ainsi, par exemple,

$$
X^T = 
 \begin{pmatrix}
  x_{11} & x_{21} & \cdots & x_{n1} \\
  x_{12} & x_{22} & \cdots & x_{n2} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{1p} & x_{2p} & \cdots & x_{np} 
 \end{pmatrix}
$$

Et

$$
x_i^T=(x_{i1}x_{i2}\ldots x_{ip})
$$

Nous utilisons $y_i$ pour d√©signer la $i^{√®me}$ observation de la variable sur laquelle nous souhaitons faire des pr√©dictions, comme par exemple le salaire. Par cons√©quent, nous √©crivons l'ensemble des n observations sous forme de vecteur comme suit 

$$
y = 
 \begin{pmatrix}
  y_{1}\\
  y_{2}\\
  \vdots \\
  y_{n} 
 \end{pmatrix}
$$

Alors nos donn√©es observ√©es consistent en ${(x_1,y_1),\ (x_2,y_2),...,\ (x_n,y_n)}$, o√π chaque $x_i$ est un vecteur de longueur $p$. (Si $p=1$, alors $x_i$ est simplement un scalaire.) 

Dans ce texte, un vecteur de longueur $n$ sera toujours d√©sign√© par une minuscule grasse.

$$
\mathtt{a} =
 \begin{pmatrix}
  a_{1}\\
  a_{2}\\
  \vdots \\
  a_{n} 
 \end{pmatrix}
$$

Cependant, les vecteurs qui ne sont pas de longueur $n$ (tels que les vecteurs attributs de longueur $p$) sont d√©sign√©s par une police normale minuscule, par exemple $\boldsymbol a$. 

Les scalaires sont √©galement d√©sign√©s par une police normale minuscule, par exemple $a$. Dans les rares cas o√π ces deux utilisations de la police normale minuscule conduisent √† une ambigu√Øt√©, nous pr√©cisons quelle utilisation est pr√©vue. 

Les matrices sont d√©sign√©es par des majuscules en gras, par exemple $\boldsymbol A$. 

Les variables al√©atoires sont d√©sign√©es par une police normale en majuscules, par exemple $A$, quelles que soient leurs dimensions.

Occasionnellement, nous voulons indiquer la dimension d'un objet particulier. Pour indiquer qu'un objet est un scalaire, nous utiliserons la notation $a\in\mathbb{R}$. 

Pour indiquer que c'est un vecteur de longueur $k$, nous utilisons $a\in\mathbb{R}^k$ (ou $a\in\mathbb{R}^n$ s‚Äôil est de longueur n). 

Nous indiquerons qu'un objet est une matrice $r\ \times\ s$ en utilisant $A\in\mathbb{R}^{r\ {x\ }s}$  ‚ÄÉ
‚ÄÉ
Dans la mesure du possible, nous √©vitons d'utiliser l'alg√®bre matricielle. Cependant, dans certains cas, cela devient trop lourd pour l'√©viter compl√®tement. Dans ces rares cas, il est important de comprendre le concept de multiplication de deux matrices. 

Supposons que $A\in\mathbb{R}^{r\ \times\ d}$ et $B\in\mathbb{R}^{d\ \times\ s}$. Alors le produit de $A$ et $B$ est d√©not√© $AB$. 
Le $i,j^{√®me}$ √©l√©ment de $AB$ est calcul√© en multipliant chaque √©l√©ment de la $i^{√®me}$ ligne de $A$ par l'√©l√©ment correspondant de la $j^{√®me}$ colonne de $B$.

Donc, nous avons

$$
\boxed{AB_{ij}=\sum_{k=1}^{d}{a_{ik}b_{kj}}}
$$

Soit l‚Äôexemple, 

$$
A = 
 \begin{pmatrix}
  1 & 2\\
  3 & 4
 \end{pmatrix}
 \ et \ 
\
B = 
 \begin{pmatrix}
  5 & 6\\
  7 & 8
 \end{pmatrix}
$$

Alors

$$
AB = 
 \begin{pmatrix}
  1 & 2\\
  3 & 4
 \end{pmatrix}
 \begin{pmatrix}
  5 & 6\\
  7 & 8
 \end{pmatrix}
  =\begin{pmatrix}
  1 \times 5 + 2\times 7 \hspace{2em}1 \times 6 + 2\times 8 \\
  3 \times 5 + 4\times 7 \hspace{2em}3 \times 6 + 4\times 8 
 \end{pmatrix}
  =\begin{pmatrix}
  19 & 22\\
  43 & 50
 \end{pmatrix}
$$

Notez que cette op√©ration produit une matrice $r\ \times\ s$. Il n'est possible de calculer $AB$ que si le nombre de colonnes de $A$ est √©gal au nombre de lignes de $B$.



<a name="B"/>

## [-B- Calcul des d√©riv√©es partielles: descente de gradient](#b) ##

[Retour TOC](#toc)

Pour impl√©menter l‚Äôalgorithme de Gradient Descent, il faut calculer les d√©riv√©es partielles de la Fonction de Co√ªt. Pour rappel, en math√©matique, la d√©riv√©e d‚Äôune fonction en un point nous donne la valeur de sa pente en ce point.

Fonction Co√ªt : 




$$
J(a, b) = \frac{1}{2m} \sum_{i=1}^{m} (ax_i + b - y_i)^2
$$




D√©riv√©e selon le param√®tre $a$ : 




$$
\frac{\partial J(a,b)}{\partial a} = \frac{1}{m} \sum_{i=1}^{m} (ax_i + b - y_i) \times x_i
$$




D√©riv√©e selon le param√®tre ùíÉ :




$$
\frac{\partial J(a,b)}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (ax_i + b - y_i)
$$




Nous somme dans le cas de la d√©riv√©e d‚Äôune fonction compos√©e : 




$$
(ùëî ‚àò ùëì)‚Ä≤ = ùëì‚Ä≤ √ó ùëî‚Ä≤ ‚àò ùëì. Avec : ùíá = ùíÇùíô + ùíÉ ‚àí ùíö\ et\ ùíà = (ùíá)^ùüê .
$$


En d√©rivant, le carr√© tombe et se simplifie avec la fraction $\frac{1}{2m}$ pour devenir $\frac{1}{m}$ et $x_i$ apparait en facteur pour la d√©riv√©e par rapport √† $a$.

<a name="C"/>

## [-C- Alg√®bre Lin√©aire](#c) ##

R√©vision et r√©f√©rence en alg√®bre lin√©aire [[19]](https://see.stanford.edu/Course/CS229)

<a name="C-1"/>

### [C.1 Concepts et notations de base](#c-1) ###

[Retour TOC](#toc)

L'alg√®bre lin√©aire permet de repr√©senter de mani√®re compacte et d'op√©rer sur des ensembles d'√©quations lin√©aires. Par exemple, consid√©rons le syst√®me d'√©quations suivant :




$$
\begin{aligned}
4 x_1-5 x_2 & =-13 \\
-2 x_1+3 x_2 & =9 .
\end{aligned}
$$


Il s'agit de deux √©quations et de deux variables. Comme vous l'avez appris en cours d'alg√®bre au lyc√©e, vous pouvez trouver une solution unique pour $x_1$ et $x_2$ (sauf si les √©quations sont d√©g√©n√©r√©es d'une mani√®re ou d'une autre, par exemple si la deuxi√®me √©quation est simplement un multiple de la premi√®re, mais dans le cas ci-dessus, il existe en fait une solution unique). En notation matricielle, nous pouvons √©crire le syst√®me de mani√®re plus compacte comme suit :




$$
\begin{aligned}
& A x=b \\
& \text { with } A=\left[\begin{array}{cc}
4 & -5 \\
-2 & 3
\end{array}\right], b=\left[\begin{array}{c}
13 \\
-9
\end{array}\right] \text {. } \\
&
\end{aligned}
$$




Comme nous le verrons bient√¥t, l'analyse des √©quations lin√©aires sous cette forme pr√©sente de nombreux avantages (y compris un gain de place √©vident).

<a name="C-1-1"/>

#### [C.1.1 Notation de base](#c-1-1) ####

[Retour TOC](#toc)

Nous utilisons la notation suivante :

- Par $A \in \mathbb{R}^{m \times n}$ nous d√©signons une matrice avec $m$ lignes et $n$ colonnes, o√π les entr√©es de $A$ sont des nombres r√©els.

- Par $x \in \mathbb{R}^n$, on d√©signe un vecteur √† $n$ entr√©es. Habituellement, un vecteur $x$ d√©signera un ***vecteur colonne -*** c'est-√†-dire une matrice √† $n$ lignes et 1 colonne. 

  Si nous voulons repr√©senter explicitement un ***vecteur ligne*** - une matrice avec 1 ligne et $n$ colonnes - nous √©crivons typiquement $x^T$ (ici $x^T$ d√©signe la transpos√©e de $x$, que nous d√©finirons bient√¥t).

- L'√©l√©ment $i$ d'un vecteur $x$ est not√© $x_i$ :




$$
x=\left[\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}\right] .
$$




- Nous utilisons la notation $a_{i j}$ (ou $A_{i j}, A_{i, j}$, etc) pour d√©signer l'entr√©e de $A$ dans la $i^{√®me}$ ligne et la $j^{√®me}$ colonne :




$$
A=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m n}
\end{array}\right] .
$$




- Nous d√©signons la $j^{√®me}$ colonne de $A$ par $a_j$ ou $A_{ :, j}$ :




$$
A=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
a_1 & a_2 & \cdots & a_n \\
\mid & \mid & & \mid
\end{array}\right] .
$$




- Nous d√©signons la $i^{√®me}$ ligne de $A$ par $a_i^T$ ou $A_{i,:}$ :




$$
A=\left[\begin{array}{ccc}
{-} & a_1^T & {-} \\
{-} & a_2^T & {-} \\
& \vdots & \\
{-} & a_m^T & {-}
\end{array}\right]
$$




- Notez que ces d√©finitions sont ambigu√´s (par exemple, les $a_1$ et $a_1^T$ dans les deux d√©finitions pr√©c√©dentes ne sont pas le m√™me vecteur). En g√©n√©ral, la signification de la notation devrait √™tre √©vidente √† partir de son utilisation.

<a name="C-2"/>

### [C.2 Multiplication matricielle](#c-2) ###

[Retour TOC](#toc)

Le produit de deux matrices $A \in \mathbb{R}^{m \times n}$ et $B \in \mathbb{R}^{n \times p}$ est la matrice




$$
C=A B \in \mathbb{R}^{m \times p},
$$




o√π




$$
C_{i j}=\sum_{k=1}^n A_{i k} B_{k j} .
$$




Notez que pour que le produit matriciel existe, le nombre de colonnes de $A$ doit √™tre √©gal au nombre de lignes de $B$. Il existe de nombreuses fa√ßons de consid√©rer la multiplication matricielle, et nous allons commencer par examiner quelques cas particuliers.

<a name="C-2-1"/>

#### [C.2.1 Produits vectoriels](#c-2-1) ####

[Retour TOC](#toc)

√âtant donn√© deux vecteurs $x, y \in \mathbb{R}^n$, la quantit√© $x^T y$, parfois appel√©e produit interne ou produit scalaire des vecteurs, est un nombre r√©el donn√© par




$$
x^T y \in \mathbb{R}=\sum_{i=1}^n x_i y_i .
$$




Notez que c'est toujours le cas que $x^T y=y^T x$.

√âtant donn√© les vecteurs $x \in \mathbb{R}^m, y \in \mathbb{R}^n$ (il n'est plus n√©cessaire qu'ils aient la m√™me taille), $x y^T$ est appel√© le produit externe des vecteurs. Il s'agit d'une matrice dont les entr√©es sont donn√©es par $\left(x y^T\right)_{i j}=x_i y_j$, c'est-√†-dire,




$$
x y^T \in \mathbb{R}^{m \times n}=\left[\begin{array}{cccc}
x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
\vdots & \vdots & \ddots & \vdots \\
x_m y_1 & x_m y_2 & \cdots & x_m y_n
\end{array}\right]
$$


<a name="C-2-2"/>

#### [C.2.2 Produits matrice-vecteur](#c-2-2) ####

[Retour TOC](#toc)

√âtant donn√© une matrice $A \in \mathbb{R}^{m \times n}$ et un vecteur $x \in \mathbb{R}^n$, leur produit est un vecteur $y=A x \in \mathbb{R}^m$. Il existe deux fa√ßons de consid√©rer la multiplication matrice-vecteur, et nous allons les examiner toutes les deux.

Si nous √©crivons $A$ par lignes, nous pouvons alors exprimer $A x$ comme,




$$
y=\left[\begin{array}{ccc}
{-} & a_1^T & {-} \\
{-} & a_2^T & {-} \\
\vdots & \\
{-} & a_m^T & {-}
\end{array}\right] x=\left[\begin{array}{c}
a_1^T x \\
a_2^T x \\
\vdots \\
a_m^T x
\end{array}\right] .
$$




En d'autres termes, la $i^{√®me}$ entr√©e de $y$ est √©gale au produit interne de la $i^{√®me}$ ligne de $A$ et de $x$, $y_i=a_i^T x$.

On peut aussi √©crire $A$ sous forme de colonnes. Dans ce cas, nous voyons que,




$$
y=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
a_1 & a_2 & \cdots & a_n \\
\mid & \mid & & \mid
\end{array}\right]\left[\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}\right]=\left[a_1\right] x_1+\left[\begin{array}{c}
a_2 \\
x_2+\ldots+\left[x_n\right.
\end{array}\right] x_n
$$


En d'autres termes, y est une combinaison lin√©aire des colonnes de $A$, o√π les coefficients de la combinaison lin√©aire sont donn√©s par les entr√©es de $x$.

Jusqu'√† pr√©sent, nous avons multipli√© √† droite par un vecteur colonne, mais il est √©galement possible de multiplier √† gauche par un vecteur ligne. Cela s'√©crit $y^T=x^T A$ pour $A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^m$, et $y \in \mathbb{R}^n$. Comme pr√©c√©demment, nous pouvons exprimer $y^T$ de deux mani√®res √©videntes, selon que nous exprimons $A$ en termes sur ses lignes ou ses colonnes.

Dans le premier cas, nous exprimons $A$ en termes de ses colonnes, ce qui donne




$$
y^T=x^T\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
a_1 & a_2 & \cdots & a_n \\
\mid & \mid & & \mid
\end{array}\right]=\left[\begin{array}{llll}
x^T a_1 & x^T a_2 & \cdots & x^T a_n
\end{array}\right]
$$




ce qui d√©montre que la $i$ i√®me entr√©e de $y^T$ est √©gale au produit interne de $x$ et de la $i$ i√®me colonne de $A$.

Enfin, en exprimant $A$ en termes de lignes, nous obtenons la repr√©sentation finale du produit vecteur-matrice,




$$
\begin{aligned}
y^T & =\left[\begin{array}{llll}
x_1 & x_2 & \cdots & x_n
\end{array}\right]\left[\begin{array}{ccc}
{-} &a_1^T & {-} \\
{-} &a_2^T & {-} \\
\vdots & \\
{-}&a_m^T & {-}
\end{array}\right] \\
& =x_1\left[-a_1^T-\right]+x_2\left[\begin{array}{lll}
{-} & a_2^T & {-}
\end{array}\right]+\ldots+x_n\left[\begin{array}{lll}
{-} & a_n^T{-}
\end{array}\right]
\end{aligned}
$$




Nous voyons donc que $y^T$ est une combinaison lin√©aire des lignes de $A$, o√π les coefficients de la combinaison lin√©aire sont donn√©s par les entr√©es de $x$.

<a name="C-2-3"/>

#### [C.2.3 Produits matriciels](#c-2-3) ####

[Retour TOC](#toc)

Forts de ces connaissances, nous pouvons maintenant examiner quatre fa√ßons diff√©rentes (mais bien s√ªr √©quivalentes) de consid√©rer la multiplication matrice-matrice $C=A B$ telle que d√©finie au d√©but de cette section. Tout d'abord, nous pouvons consid√©rer la multiplication matrice-matrice comme un ensemble de produits vecteur-vecteur. Le point de vue le plus √©vident, qui d√©coule imm√©diatement de la d√©finition, est que l'entr√©e $i, j$ de $C$ est √©gale au produit interne de la $i$√®me ligne de $A$ et de la $j$√®me ligne de $B$. Symboliquement, cela ressemble √† ce qui suit,




$$
C=A B=\left[\begin{array}{cc}
{-} & a_1^T \\
{-} & a_2^T \\
\vdots \\
{-} & a_m^T
\end{array}\right]\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
b_1 & b_2 & \cdots & b_p \\
\mid & \mid & & \mid
\end{array}\right]=\left[\begin{array}{cccc}
a_1^T b_1 & a_1^T b_2 & \cdots & a_1^T b_p \\
a_2^T b_1 & a_2^T b_2 & \cdots & a_2^T b_p \\
\vdots & \vdots & \ddots & \vdots \\
a_m^T b_1 & a_m^T b_2 & \cdots & a_m^T b_p
\end{array}\right]
$$




Rappelez-vous que puisque $A \in \mathbb{R}^{m \times n}$ et $B \in \mathbb{R}^{n \times p}, a_i \in \mathbb{R}^n$ et $b_j \in \mathbb{R}^n$, ces produits internes ont tous un sens. C'est la repr√©sentation la plus "naturelle" lorsque nous repr√©sentons $A$ par des lignes et $B$ par des colonnes. Alternativement, nous pouvons repr√©senter $A$ par des colonnes et $B$ par des lignes, ce qui conduit √† l'interpr√©tation de $A B$ comme une somme de produits externes. Symboliquement,




$$
C=A B=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
a_1 & a_2 & \cdots & a_n \\
\mid & \mid & & \mid
\end{array}\right]\left[\begin{array}{ccc}
{-} & b_1^T & {-} \\
{-} & b_2^T & {-} \\
\vdots \\
{-} & b_n^T & {-}
\end{array}\right]=\sum_{i=1}^n a_i b_i^T .
$$


Autrement dit, $A B$ est √©gal √† la somme, sur tous les $i$, du produit externe de la $i^{√®me}$ colonne de $A$ et de la $i$ √®me ligne de $B$. Puisque, dans ce cas, $a_i \in \mathbb{R}^m$ et $b_i \in \mathbb{R}^p$, la dimension du produit ext√©rieur $a_i b_i^T$ est $m \times p$, ce qui co√Øncide avec la dimension de $C$.

Deuxi√®mement, nous pouvons √©galement consid√©rer la multiplication matrice-matrice comme un ensemble de produits matrice-vecteur. Plus pr√©cis√©ment, si nous repr√©sentons $B$ par des colonnes, nous pouvons consid√©rer les colonnes de $C$ comme des produits matrice-vecteur entre $A$ et les colonnes de $B$. Symboliquement,




$$
C=A B=A\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
b_1 & b_2 & \cdots & b_p \\
\mid & \mid & & \mid
\end{array}\right]=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
A b_1 & A b_2 & \cdots & A b_p \\
\mid & \mid & & \mid
\end{array}\right] .
$$




Ici, la colonne $i$ de $C$ est donn√©e par le produit matrice-vecteur avec le vecteur de droite, $c_i=A b_i$. Ces produits matrice-vecteur peuvent √† leur tour √™tre interpr√©t√©s en utilisant les deux points de vue donn√©s dans la sous-section pr√©c√©dente. Enfin, nous avons le point de vue analogue, o√π nous repr√©sentons $A$ par des lignes, et voyons les lignes de $C$ comme le produit matrice-vecteur entre les lignes de $A$ et $C$. Symboliquement,




$$
C=A B=\left[\begin{array}{cc}
{-} & a_1^T \\
{-} & a_2^T \\
\vdots \\
{-} & a_m^T
\end{array}\right] B=\left[\begin{array}{ccc}
{-} & a_1^T B & {-} \\
{-} & a_2^T B & {-} \\
\vdots & \\
{-} & a_m^T B & {-}
\end{array}\right]
$$




Ici la $i^{√®me}$ ligne de $C$ est donn√©e par le produit matrice-vecteur avec le vecteur de gauche, $c_i^T=a_i^T B$.

Il peut sembler exag√©r√© de diss√©quer la multiplication matricielle √† un tel degr√©, surtout lorsque tous ces points de vue d√©coulent imm√©diatement de la d√©finition initiale que nous avons donn√©e (en une ligne de math√©matiques environ) au d√©but de cette section. Cependant, pratiquement toute l'alg√®bre lin√©aire traite des multiplications matricielles d'une mani√®re ou d'une autre, et il vaut la peine de passer un peu de temps √† essayer de d√©velopper une compr√©hension intuitive des points de vue pr√©sent√©s ici.

En plus de cela, il est utile de conna√Ætre quelques propri√©t√©s de base de la multiplication matricielle √† un niveau sup√©rieur :

- La multiplication matricielle est associative : $(A B) C=A(B C)$.

- La multiplication matricielle est distributive : $A(B+C)=A B+A C$.

- La multiplication matricielle n'est, en g√©n√©ral, pas commutative, c'est-√†-dire qu'il peut arriver que $A B \neq B A$.

<a name="C-3"/>

### [C.3 Op√©rations et propri√©t√©s](#c-3) ###

[Retour TOC](#toc)

Dans cette section, nous pr√©sentons plusieurs op√©rations et propri√©t√©s des matrices et des vecteurs. Nous esp√©rons qu'une grande partie de ces notions vous sera famili√®re et que les notes serviront juste de r√©f√©rence pour ces sujets.

<a name="C-3-1"/>

#### [C.3.1 Matrice d'identit√© et matrices diagonales](#c-3-1) ####

[Retour TOC](#toc)

La matrice d'identit√©, not√©e $I \in \mathbb{R}^{n \times n}$, est une matrice carr√©e avec des uns sur la diagonale et des z√©ros partout ailleurs. C'est-√†-dire ,




$$
I_{i j}= \begin{cases}1 & i=j \\ 0 & i \neq j\end{cases}
$$




Il a la propri√©t√© que pour tout $A \in \mathbb{R}^{m \times n}$,




$$
A I=A=I A
$$


o√π la taille de $I$ est d√©termin√©e par les dimensions de $A$ de sorte que la multiplication de la matrice est possible.

Une matrice diagonale est une matrice dont tous les √©l√©ments non diagonaux sont 0 . Elle est g√©n√©ralement not√©e $D={diag}\left(d_1, d_2, \ldots, d_n\right)$, avec




$$
D_{i j}= \begin{cases}d_i & i=j \\ 0 & i \neq j\end{cases}
$$


Clairement, $I={diag}(1,1, \ldots, 1)$.

<a name="C-3-2"/>

#### [C.3.2 La transposition](#c-3-2) ####

[Retour TOC](#toc)

La transposition d'une matrice r√©sulte de la " permutation " des lignes et des colonnes. √âtant donn√© une matrice $A \in \mathbb{R}^{m \times n}$, sa transposition, √©crite $A^T$, est d√©finie comme suit




$$
A^T \in \mathbb{R}^{n \times m},\left(A^T\right)_{i j}=A_{j i} .
$$




En fait, nous avons d√©j√† utilis√© la transposition pour d√©crire les vecteurs lignes, puisque la transposition d'un vecteur colonne est naturellement un vecteur ligne.

Les propri√©t√©s suivantes des transpositions sont facilement v√©rifi√©es :

- $\left(A^T\right)^T=A$

- $(A B)^T=B^T A^T$

- $(A+B)^T=A^T+B^T$

<a name="C-3-3"/>

#### [C.3.3 Matrices sym√©triques](#c-3-3) ####

[Retour TOC](#toc)

Une matrice carr√©e $A \in \mathbb{R}^{n \times n}$ est sym√©trique si $A=A^T$. Elle est antisym√©trique si $A=-A^T$. Il est facile de montrer que pour toute matrice $A \in \mathbb{R}^{n \times n}$, la matrice $A+A^T$ est sym√©trique et la matrice $A-A^T$ est antisym√©trique. Il s'ensuit que toute matrice carr√©e $A \in \mathbb{R}^{n \times n}$ peut √™tre repr√©sent√©e comme une somme d'une matrice sym√©trique et d'une matrice antisym√©trique, puisque




$$
A=\frac{1}{2}\left(A+A^T\right)+\frac{1}{2}\left(A-A^T\right)
$$




et la premi√®re matrice √† droite est sym√©trique, tandis que la seconde est antisym√©trique. Il s'av√®re que les matrices sym√©triques sont tr√®s fr√©quentes dans la pratique et qu'elles poss√®dent de nombreuses propri√©t√©s int√©ressantes que nous allons examiner sous peu. Il est courant de d√©signer l'ensemble de toutes les matrices sym√©triques de taille $n$ par $\mathbb{S}^n$, de sorte que $A \in \mathbb{S}^n$ signifie que $A$ est une matrice sym√©trique $n \times n$ ;

<a name="C-3-4"/>

#### [C.3.4 La trace](#c-3-4) ####

[Retour TOC](#toc)

La trace d'une matrice carr√©e $A \in \mathbb{R}^{n \times n}$, not√©e ${tr}(A)$ (ou juste ${tr} A$ si les parenth√®ses sont √©videmment implicites), est la somme des √©l√©ments diagonaux de la matrice :




$$
{tr} A=\sum_{i=1}^n A_{i i} .
$$




Comme d√©crit dans les notes de cours de CS229, la trace a les propri√©t√©s suivantes (incluses ici par souci d'exhaustivit√©) :

- Pour $A \in \mathbb{R}^{n \times n}, {tr} A={tr} A^T$.

- Pour $A, B \in \mathbb{R}^{n \times n}, {tr}(A+B)={tr} A+{tr} B$.

- Pour $A \in \mathbb{R}^{n \times n}, t\ dans\ \mathbb{R}, {tr}(t A)=t {tr} A$.

- Pour $A, B$ tels que $A B$ est carr√©, ${tr} A B={tr} B A$.

\- Pour $A, B, C$ tels que $A B C$ est carr√©, ${tr} A B C={tr} B C A={tr} C A B$, et ainsi de suite pour le produit de plusieurs matrices.

<a name="C-3-5"/>

#### [C.3.5 Normes](#c-3-5) ####

[Retour TOC](#toc)

La norme d'un vecteur $\|x\|$ est une mesure informelle de la "longueur" du vecteur. Par exemple, nous disposons de la norme euclidienne ou $\ell_2$ commun√©ment utilis√©e,




$$
\|x\|_2=\sqrt{\sideset{}{^n_{i=1}}\sum x_i^2}
$$




Notez que $\|x\|_2^2=x^T x$.

Plus formellement, une norme est toute fonction $f : \mathbb{R}^n \rightarrow \mathbb{R}$ qui satisfait 4 propri√©t√©s :

1. Pour tout $x \in \mathbb{R}^n, f(x) \geq 0$ (non-n√©gativit√©).

2. $f(x)=0$ si et seulement si $x=0$ (d√©finitude).

3. Pour tout $x \in \mathbb{R}^n, t \in \mathbb{R}, f(t x)=|t| f(x)$ (homog√©n√©it√©).

4. Pour tout $x, y \in \mathbb{R}^n, f(x+y) \leq f(x)+f(y)$ (in√©galit√© triangulaire).

D'autres exemples de normes sont la norme $\ell_1$,




$$
\|x\|_1=\sum_{i=1}^n\left|x_i\right|
$$




et la norme $\ell_{\infty}$,




$$
\|x\|_{\infty}=\max _i\left|x_i\right| .
$$




En fait, les trois normes pr√©sent√©es jusqu'ici sont des exemples de la famille des normes $\ell_p$, qui sont param√©tr√©es par un nombre r√©el $p \geq 1$, et d√©finies comme suit




$$
\|x\|_p=\biggl(\sideset{}{^n_{i=1}}\sum |x_i|^p\biggl)^{1 / p}
$$




Des normes peuvent √©galement √™tre d√©finies pour les matrices, comme la norme de Frobenius,




$$
\|A\|_F=\sqrt{\sideset{}{^m_{i=1}}\sum \sideset{}{^n_{j=1}}\sum A_{i j}^2}=\sqrt{{tr}(A^T A)}
$$




De nombreuses autres normes existent, mais elles d√©passent le cadre de cet examen.

<a name="C-3-6"/>

#### [C.3.6 Ind√©pendance lin√©aire et rang](#c-3-6) ####

[Retour TOC](#toc)

Un ensemble de vecteurs $\{x_1, x_2, \ldots x_n\}$ est dit (lin√©airement) ind√©pendant si aucun vecteur ne peut √™tre repr√©sent√© comme une combinaison lin√©aire des vecteurs restants. Inversement, un vecteur qui peut √™tre repr√©sent√© comme une combinaison lin√©aire des vecteurs restants est dit (lin√©airement) d√©pendant. Par exemple, si




$$
x_n=\sideset{}{^{n-1}_{i=1}}\sum \alpha_i x_i
$$




pour un certain $\{\alpha_1, \ldots, \alpha_{n-1}\}$ alors $x_n$ est d√©pendant de $\{x_1, \ldots, x_{n-1}\}$ ; sinon, il est ind√©pendant de $\{x_1, \ldots, x_{n-1}\}$.

La colonne ${rank}$ d'une matrice $A$ est le plus grand nombre de colonnes de $A$ qui constituent un ensemble lin√©airement ind√©pendant. On l'appelle souvent simplement le nombre de colonnes lin√©airement ind√©pendantes, mais cette terminologie est un peu n√©glig√©e, car il est possible que tout vecteur d'un ensemble $\{x_1, \ldots x_n\}$ puisse √™tre exprim√© comme une combinaison lin√©aire des vecteurs restants, m√™me si un sous-ensemble des vecteurs peut √™tre ind√©pendant. De la m√™me mani√®re, le rang est le plus grand nombre de rangs de $A$ qui constituent un ensemble lin√©airement ind√©pendant.

C'est un fait de base de l'alg√®bre lin√©aire, que pour toute matrice $A, {columnrank}(A)={rowrank}(A)$, et donc cette quantit√© est simplement d√©sign√©e comme le ${rank}$ de $A$, not√© ${rank}(A)$. Voici quelques propri√©t√©s de base du rang :

- Pour $A \in \mathbb{R}^{m \times n}, {rank}(A) \leq \min (m, n)$. Si ${rank}(A)=\min (m, n)$, alors $A$ est dit de rang complet.

- Pour $A \in \mathbb{R}^{m \times n}, {rank}(A)={rank}\left(A^T\right)$.

- Pour $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}, {rank}(A B) \leq \min ({rank}(A), {rank}(B))$.

- Pour $A, B \in \mathbb{R}^{m \times n}, {rank}(A+B) \leq {rank}(A)+{rank}(B)$.

<a name="C-3-7"/>

#### [C.3.7 L'inverse](#c-3-7) ####

[Retour TOC](#toc)

L'inverse d'une matrice carr√©e $A \in \mathbb{R}^{n \times n}$ est not√© $A^{-1}$, et est la seule matrice telle que




$$
A^{-1} A=I=A A^{-1} .
$$




Il s'av√®re que $A^{-1}$ peut ne pas exister pour certaines matrices $A$ ; on dit que $A$ est inversible ou non singuli√®re si $A^{-1}$ existe et non inversible ou singuli√®re sinon. Nous connaissons d√©j√† une condition d'inversibilit√© : il est possible de montrer que $A^{-1}$ existe si et seulement si $A$ est de rang complet. Nous verrons bient√¥t qu'il existe de nombreuses autres conditions suffisantes et n√©cessaires, en plus du rang complet, pour l'inversibilit√©. Les propri√©t√©s suivantes sont des propri√©t√©s de l'inverse ; toutes supposent que $A, B\ dans \mathbb{R}^{n \times n}$ sont non-singuliers :

- $(A^{-1})^{-1}=A$

- Si $A x=b$, nous pouvons multiplier par $A^{-1}$ des deux c√¥t√©s pour obtenir $x=A^{-1} b$. Ceci d√©montre l'inverse par rapport au syst√®me original d'√©galit√©s lin√©aires avec lequel nous avons commenc√© cette revue.
- $(A B)^{-1}=B^{-1} A^{-1}$

- $(A^{-1})^T=(A^T)^{-1}$. Pour cette raison, cette matrice est souvent not√©e $A^{-T}$.

<a name="C-3-8"/>

#### [C.3.8 Matrices orthogonales](#c-3-8) ####

[Retour TOC](#toc)

Deux vecteurs $x, y \in \mathbb{R}^n$ sont orthogonaux si $x^T y=0$. Un vecteur $x \in \mathbb{R}^n$ est normalis√© si $\|x\|_2=1$. Une matrice carr√©e $U \in \mathbb{R}^{n \times n}$ est orthogonale (notez les diff√©rentes significations lorsque vous parlez de vecteurs par rapport aux matrices) si toutes ses colonnes sont orthogonales les unes aux autres et sont normalis√©es (les colonnes sont alors dites orthonormales).

Il d√©coule imm√©diatement de la d√©finition de l'orthogonalit√© et de la normalit√© que




$$
U^T U=I=U U^T .
$$




En d'autres termes, l'inverse d'une matrice orthogonale est sa transpos√©e. Notez que si $U$ n'est pas carr√©e - c'est-√†-dire $U \in \mathbb{R}^{m \times n}, n < m$ - mais que ses colonnes sont toujours orthonormales, alors $U^T U=I$, mais $U U^T \neq I$. Nous n'utilisons g√©n√©ralement le terme orthogonal que pour d√©crire le cas pr√©c√©dent, o√π $U$ est carr√©.

Une autre propri√©t√© int√©ressante des matrices orthogonales est que le fait d'op√©rer sur un vecteur avec une matrice orthogonale ne changera pas sa norme euclidienne, c'est-√†-dire..,




$$
\|U x\|_2=\|x\|_2
$$




pour tout $x \in \mathbb{R}^n, U \in \mathbb{R}^{n \times n}$ orthogonal.

<a name="C-3-9"/>

#### [C.3.9 Plage et nulspace d'une matrice](#c-3-9) ####

[Retour TOC](#toc)

L'√©tendue d'un ensemble de vecteurs $\{x_1, x_2, \ldots x_n\}$ est l'ensemble de tous les vecteurs qui peuvent √™tre exprim√©s comme une combinaison lin√©aire de $\{x_1, \ldots, x_n\}$. C'est-√†-dire,




$$
{span}\left(\{x_1, \ldots x_n\}\right)=\{v: v=\sideset{}{^n_{i=1}}\sum \alpha_i x_i, \quad \alpha_i \in \mathbb{R}\} .
$$




On peut montrer que si $\{x_1, \ldots, x_n\}$ est un ensemble de $n$ vecteurs lin√©airement ind√©pendants, o√π chaque $x_i \in \mathbb{R}^n$, alors l'√©tendue $(\{x_1, \ldots x_n\})=\mathbb{R}^n$. En d'autres termes, tout vecteur $v\ dans\ \mathbb{R}^n$ peut √™tre √©crit comme une combinaison lin√©aire de $x_1$ √† $x_n$. La projection d'un vecteur $y\ dans\ \mathbb{R}^m$ sur l'√©tendue de $\{x_1, \ldots, x_n\}$ (nous supposons ici $\left. x_i \ dans \mathbb{R}^m\right)$ est le vecteur $v \in {span}(\{x_1, \ldots x_n\})$, tel que $v$ soit aussi proche que possible de $y$, mesur√© par la norme euclidienne $\|v-y|_2$. Nous d√©signons la projection par ${Proj}(y ;\{x_1, \ldots, x_n\})$ et pouvons la d√©finir formellement comme,




$$
{Proj}(y ;\{x_1, \ldots x_n\})={argmin}_{v \in {span}(\{x_1, \ldots, x_n\})}\|y-v\|_2 .
$$




L'√©tendue (parfois aussi appel√©e espace des colonnes) d'une matrice $A \in \mathbb{R}^{m \times n}$, not√©e $\mathcal{R}(A)$, est l'√©tendue des colonnes de $A$. En d'autres termes,




$$
\mathcal{R}(A)=\{v \in \mathbb{R}^m: v=A x, x \in \mathbb{R}^n\} .
$$




En faisant quelques hypoth√®ses techniques (√† savoir que $A$ est de rang complet et que $n < m$ ), la projection d'un vecteur $y \in \mathbb{R}^m$ sur l'intervalle de $A$ est donn√©e par,




$$
{Proj}(y ; A)={argmin}_{v \in \mathcal{R}(A)}\|v-y\|_2=A(A^T A)^{-1} A^T y .
$$




Cette derni√®re √©quation devrait vous sembler extr√™mement famili√®re, puisqu'il s'agit presque de la m√™me formule que nous avons d√©riv√©e en classe (et que nous allons bient√¥t d√©river √† nouveau) pour l'estimation des param√®tres par les moindres carr√©s. En regardant la d√©finition de la projection, il ne devrait pas √™tre trop difficile de vous convaincre qu'il s'agit en fait du m√™me objectif que celui que nous avons minimis√© dans notre probl√®me des moindres carr√©s (√† l'exception d'un quadrillage de la norme, qui n'affecte pas le point optimal) et que ces probl√®mes sont donc naturellement tr√®s li√©s. Lorsque $A$ ne contient qu'une seule colonne, $a \in \mathbb{R}^m$, cela donne le cas particulier de la projection d'un vecteur sur une droite :




$$
{Proj}(y ; a)=\frac{a a^T}{a^T a} y .
$$




L'espace nul d'une matrice $A \in \mathbb{R}^{m \times n}$, not√© $\mathcal{N}(A)$ est l'ensemble de tous les vecteurs qui sont √©gaux √† 0 lorsqu'ils sont multipli√©s par $A$, c'est √† dire,




$$
\mathcal{N}(A)=\{x \in \mathbb{R}^n: A x=0\} .
$$




Notez que les vecteurs dans $\mathcal{R}(A)$ sont de taille $m$, alors que les vecteurs dans $\mathcal{N}(A)$ sont de taille $n$, donc les vecteurs dans $\mathcal{R}\left(A^T\right)$ et $\mathcal{N}(A)$ sont tous deux dans $\mathbb{R}^n$. En fait, nous pouvons dire beaucoup plus. Il s'av√®re que




$$
\{w: w=u+v, u \in \mathcal{R}(A^T), v \in \mathcal{N}(A)\}=\mathbb{R}^n \text { and } \mathcal{R}(A^T) \cap \mathcal{N}(A)=\emptyset \text {. }
$$




En d'autres termes, $\mathcal{R}\left(A^T\right)$ et $\mathcal{N}(A)$ sont des sous-ensembles disjoints qui couvrent ensemble l'espace entier de $\mathbb{R}^n$. Les ensembles de ce type sont appel√©s des compl√©ments orthogonaux, et nous d√©signons par $\mathcal{R}\left(A^T\right)=$ $\mathcal{N}(A)^{\perp}$ les compl√©ments orthogonaux.

<a name="C-3-10"/>

#### [C.3.10 Le d√©terminant](#c-3-10) ####

[Retour TOC](#toc)

Le d√©terminant d'une matrice carr√©e $A \in \mathbb{R}^{n \times n}$, est une fonction ${det}$ : $\mathbb{R}^{n \times n}$ . $\mathbb{R}$, et est not√©e $|A|$ ou ${det} A$ (comme pour l'op√©rateur de trace, nous omettons g√©n√©ralement les parenth√®ses). La formule compl√®te du d√©terminant donne peu d'intuition sur sa signification, aussi nous donnons d'abord trois propri√©t√©s d√©terminantes du d√©terminant, dont tout le reste d√©coule (y compris la formule g√©n√©rale) :

1. Le d√©terminant de l'identit√© est $1,|I|=1$.

2. √âtant donn√© une matrice $A \in \mathbb{R}^{n \times n}$, si nous multiplions une seule ligne de $A$ par un scalaire $t \in \mathbb{R}$, alors le d√©terminant de la nouvelle matrice est $t|A|$,

3. Si nous √©changeons deux lignes quelconques $a_i^T$ et $a_j^T$ de $A$, alors le d√©terminant de la nouvelle matrice est $-|A|$, par exemple




$$
\left|\left[\begin{array}{ccc}
{-} & a_2^T & {-} \\
{-} & a_1^T & {-} \\
\vdots & \\
{-} & a_m^T & {-}
\end{array}\right]\right|=-|A|
$$




Cependant, ces propri√©t√©s ne donnent √©galement que tr√®s peu d'intuition sur la nature du d√©terminant. Nous allons donc maintenant √©num√©rer plusieurs propri√©t√©s qui d√©coulent des trois propri√©t√©s ci-dessus :

- Pour $A \in \mathbb{R}^{n \times n},|A|=|A^T|$.

- Pour $A, B \in \mathbb{R}^{n \times n},|A B|=|A||B|$.

- Pour $A \in \mathbb{R}^{n \times n},|A|=0$ si et seulement si $A$ est singulier (i.e., non-invertible).

- Pour $A \in \mathbb{R}^{n \times n}$ et $A$ non singulier, $|A|^{-1}=1 /|A|$.

Avant de donner la d√©finition g√©n√©rale du d√©terminant, on d√©finit, pour $A \in \mathbb{R}^{n \times n}, A_{\backslash i \backslash j} \in$ $\mathbb{R}^{(n-1) \times(n-1)}$ la matrice qui r√©sulte de la suppression de la $i$ $i^{√®me}$ ligne et de la $j^{√®me}$ colonne de $A$. La formule g√©n√©rale (r√©cursive) du d√©terminant est la suivante




$$
\begin{aligned}
|A| & =\sideset{}{^n_{i=1}}\sum(-1)^{i+j} a_{i j}\left|A_{\backslash i, \backslash j}\right| \quad \text { (for any } j \in 1, \ldots, n \text { ) } \\
& =\sideset{}{^n_{j=1}}\sum(-1)^{i+j} a_{i j}\left|A_{\backslash i, \backslash j}\right| \quad \text { (for any } i \in 1, \ldots, n \text { ) }
\end{aligned}
$$




avec le cas initial que $|A|=a_{11}$ pour $A \in \mathbb{R}^{1 \times 1}$. Si nous devions d√©velopper cette formule compl√®tement pour $A \in \mathbb{R}^{n \times n}$, il y aurait un total de $n$ ! ( $n$ factoriel) termes diff√©rents. Pour cette raison, nous n'√©crivons m√™me pas explicitement l'√©quation compl√®te du d√©terminant pour les matrices sup√©rieures √† $3 \times 3$. Cependant, les √©quations des d√©terminants des matrices jusqu'√† la taille de $3 \times 3$ sont assez courantes, et il est bon de les conna√Ætre :




$$
\begin{aligned}
&\left|\left[a_{11}\right]\right|=a_{11} \\
&\left|\left[\begin{array}{ll}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}\right]\right|= a_{11} a_{22}-a_{12} a_{21} \\
&\left|\left[\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right]\right|=\begin{array}{c}
a_{11} a_{22} a_{33}+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32} \\
-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}-a_{13} a_{22} a_{31}
\end{array}
\end{aligned}
$$




L'adjoint classique (souvent juste appel√© l'adjoint) d'une matrice $A \in \mathbb{R}^{n \times n}$, est not√© ${adj}(A)$, et d√©fini comme suit




$$
{adj}(A) \in \mathbb{R}^{n \times n}, \quad({adj}(A))_{i j}=(-1)^{i+j}\left|A_{\backslash j, \backslash i}\right|
$$




(notez l'inversion des indices $A \backslash, \backslash$ ). On peut montrer que pour tout √©l√©ment non singulier $A \in \mathbb{R}^{n \times n}$,




$$
A^{-1}=\frac{1}{|A|} {adj}(A) .
$$




Bien qu'il s'agisse d'une belle formule "explicite" pour l'inverse d'une matrice, nous devons noter que, num√©riquement, il existe en fait des moyens beaucoup plus efficaces de calculer l'inverse.

<a name="C-3-11"/>

#### [C.3.11 Formes quadratiques et matrices semi-d√©finies positives](#c-3-11) ####

[Retour TOC](#toc)

√âtant donn√© une matrice carr√©e $A \in \mathbb{R}^{n \times n}$ et un vecteur $x \in \mathbb{R}$, la valeur scalaire $x^T A x$ est appel√©e une forme quadratique. En √©crivant explicitement, on voit que




$$
x^T A x=\sideset{}{^n_{i=1}}\sum \sideset{}{^n_{j=1}}\sum A_{i j} x_i x_j .
$$


Notez que,




$$
x^T A x=(x^T A x)^T=x^T A^T x=x^T(\frac{1}{2} A+\frac{1}{2} A^T) x
$$




c'est-√†-dire que seule la partie sym√©trique de $A$ contribue √† la forme quadratique. Pour cette raison, nous supposons souvent implicitement que les matrices apparaissant dans une forme quadratique sont sym√©triques.

Nous donnons les d√©finitions suivantes :

- Une matrice sym√©trique $A \in \mathbb{S}^n$ est d√©finie positive (DP) si pour tous les vecteurs non nuls $x \in \mathbb{R}^n, x^T A x>0$. Ceci est g√©n√©ralement not√© $A \succ 0$ (ou juste $A > 0$ ), et souvent l'ensemble de toutes les matrices d√©finies positives est not√© $\mathbb{S}_{++}^n$.
- Une matrice sym√©trique $A \in \mathbb{S}^n$ est semi-d√©finie positive (PSD) si pour tous les vecteurs $x^T A x \geq$ 0 . Ceci s'√©crit $A \succeq 0$ (ou juste $A \geq 0$ ), et l'ensemble de toutes les matrices semi-d√©finies positives est souvent not√© $\mathbb{S}_{+}^n$.
- De m√™me, une matrice sym√©trique $A \in \mathbb{S}^n$ est d√©finie n√©gative (DN), not√©e $A \prec 0$ (ou juste $A<0$ ) si pour tout $x \in \mathbb{R}^n$ non nul, $x^T A x<0$.
- De m√™me, une matrice sym√©trique $A \in \mathbb{S}^n$ est semi-d√©finie n√©gative (NSD), not√©e $A \succeq 0$ (ou juste $A \leq 0$ ) si pour tout $x \in \mathbb{R}^n, x^T A x \leq 0$.
- Enfin, une matrice sym√©trique $A \in \mathbb{S}^n$ est ind√©finie, si elle n'est ni semi-d√©finie positive ni semi-d√©finie n√©gative - c'est-√†-dire s'il existe $x_1, x_2 \in \mathbb{R}^n$ tels que $x_1^T A x_1 > 0$ et $x_2^T A x_2 < 0$.

Il devrait √™tre √©vident que si $A$ est d√©finie positive, alors $-A$ est d√©finie n√©gative et vice versa. De m√™me, si $A$ est semi-d√©finie positive, alors $-A$ est semi-d√©finie n√©gative et vice versa. Si $A$ est ind√©finie, alors $-A$ l'est aussi. On peut √©galement montrer que les matrices d√©finies positives et d√©finies n√©gatives sont toujours inversibles.

Enfin, il existe un type de matrice d√©finie positive qui appara√Æt fr√©quemment et qui m√©rite donc une mention sp√©ciale. Pour toute matrice $A \in \mathbb{R}^{m \times n}$ (pas n√©cessairement sym√©trique ou m√™me carr√©e), la matrice $G=A^T A$ (parfois appel√©e matrice de Gram) est toujours semi-d√©finie positive. De plus, si $m \geq n$ (et nous supposons par commodit√© que $A$ est de rang complet), alors $G=A^T A$ est d√©finie positive.

<a name="C-3-12"/>

#### [C.3.12 Valeurs propres et vecteurs propres](#c-3-12) ####

[Retour TOC](#toc)

√âtant donn√© une matrice carr√©e $A \in \mathbb{R}^{n \times n}$, on dit que $\lambda \in \mathbb{C}$ est une valeur propre de $A$ et que $x \in \mathbb{C}^n$ est le vecteur propre correspondant si




$$
A x=\lambda x, \quad x \neq 0 .
$$




(Notez que $\lambda$ et les entr√©es de $x$ sont en fait dans $\mathbb{C}$, l'ensemble des nombres complexes, et pas juste les r√©els ; nous verrons bient√¥t pourquoi cela est n√©cessaire. Ne vous inqui√©tez pas de cette technicit√© pour l'instant, vous pouvez penser aux vecteurs complexes de la m√™me mani√®re qu'aux vecteurs r√©els.)

Intuitivement, cette d√©finition signifie que la multiplication de $A$ par le vecteur $x$ donne un nouveau vecteur qui pointe dans la m√™me direction que $x$, mais mis √† l'√©chelle par un facteur $\lambda$. Notez √©galement que pour tout vecteur propre $x \in \mathbb{C}^n$, et tout scalaire $t \in \mathbb{C}, A(c x)=c A x=c \lambda x=\lambda(c x)$, donc $c x$ est √©galement un vecteur propre. C'est pourquoi, lorsque nous parlons du "vecteur propre" associ√© √† $\lambda$, nous supposons g√©n√©ralement que le vecteur propre est normalis√© pour avoir une longueur de 1 (cela cr√©e encore une certaine ambigu√Øt√©, puisque $x$ et $-x$ seront tous deux des vecteurs propres, mais nous devons nous en accommoder).

Nous pouvons r√©√©crire l'√©quation ci-dessus pour dire que $(\lambda, x)$ est une paire valeur propre-vecteur propre de $A$ si,




$$
(\lambda I-A) x=0, \quad x \neq 0 .
$$




Mais $(\lambda I-A) x=0$ a une solution non nulle √† $x$ si et seulement si $(\lambda I-A)$ a un espace nul non vide, ce qui n'est le cas que si $(\lambda I-A)$ est singulier, c'est-√†-dire,




$$
|(\lambda I-A)|=0 \text {. }
$$




Nous pouvons maintenant utiliser la d√©finition pr√©c√©dente du d√©terminant pour d√©velopper cette expression en un (tr√®s grand) polyn√¥me en $\lambda$, o√π $\lambda$ aura le degr√© maximum $n$. Nous trouvons ensuite les $n$ racines (√©ventuellement complexes) de ce polyn√¥me pour trouver les $n$ valeurs propres $\lambda_1, \ldots, \lambda_n$. Pour trouver le vecteur propre correspondant √† la valeur propre $\lambda_i$, il suffit de r√©soudre l'√©quation lin√©aire $\left(\lambda_i I-A\right) x=0$. Il convient de noter que cette m√©thode n'est pas celle qui est r√©ellement utilis√©e dans la pratique pour calculer num√©riquement les valeurs propres et les vecteurs propres (rappelez-vous que l'expansion compl√®te du d√©terminant comporte $n$ ! termes) ; il s'agit plut√¥t d'un argument math√©matique.

Voici les propri√©t√©s des valeurs propres et des vecteurs propres (dans tous les cas, on suppose que $A \in \mathbb{R}^{n \times n}$ a des valeurs propres $\lambda_i, \ldots, \lambda_n$ et les vecteurs propres associ√©s $x_1, \ldots x_n$ ) :

- La trace d'un $A$ est √©gale √† la somme de ses valeurs propres,




$$
{tr} A=\sideset{}{^n_{i=1}}\sum \lambda_i
$$




- Le d√©terminant de $A$ est √©gal au produit de ses valeurs propres,




$$
|A|=\sideset{}{^n_{i=1}}\prod \lambda_i .
$$




- Le rang de $A$ est √©gal au nombre de valeurs propres non nulles de $A$.

- Si $A$ est non singulier, alors $1 / \lambda_i$ est une valeur propre de $A^{-1}$ √† laquelle est associ√© un vecteur propre $x_i$, c'est-√†-dire que $A^{-1} x_i=(1 / \lambda_i) x_i$.

- Les valeurs propres d'une matrice diagonale $D={diag}(d_1, \ldots d_n)$ sont juste les entr√©es diagonales $d_1, \ldots d_n$.

Nous pouvons √©crire simultan√©ment toutes les √©quations des vecteurs propres comme suit




$$
A X=X \Lambda
$$




o√π les colonnes de $X \in \mathbb{R}^{n \times n}$ sont les vecteurs propres de $A$ et $\Lambda$ est une matrice diagonale dont les entr√©es sont les valeurs propres de $A$, √† savoir,




$$
X \in \mathbb{R}^{n \times n}=\left[\begin{array}{cccc}
\mid & \mid & & \mid \\
x_1 & x_2 & \cdots & x_n \\
\mid & \mid & & \mid
\end{array}\right], \Lambda={diag}(\lambda_1, \ldots, \lambda_n)
$$




Si les vecteurs propres de $A$ sont lin√©airement ind√©pendants, alors la matrice $X$ sera inversible, donc $A=X \Lambda X^{-1}$. Une matrice qui peut √™tre √©crite sous cette forme est dite diagonalisable.

<a name="C-3-13"/>

#### [C.3.13 Valeurs propres et vecteurs propres des matrices sym√©triques](#c-3-13) ####

[Retour TOC](#toc)

Deux propri√©t√©s remarquables apparaissent lorsque l'on examine les valeurs propres et les vecteurs propres d'une matrice sym√©trique $A \in \mathbb{S}^n$. Premi√®rement, on peut montrer que toutes les valeurs propres de $A$ sont r√©elles. Deuxi√®mement, les vecteurs propres de $A$ sont orthonorm√©s, c'est-√†-dire que la matrice $X$ d√©finie ci-dessus est une matrice orthogonale (pour cette raison, nous d√©signons la matrice des vecteurs propres par $U$ dans ce cas). Nous pouvons donc repr√©senter $A$ comme $A=U \Lambda U^T$, en nous rappelant que l'inverse d'une matrice orthogonale est juste sa transpos√©e.

En utilisant ceci, nous pouvons montrer que le caract√®re d√©finitif d'une matrice d√©pend enti√®rement du signe de ses valeurs propres. Supposons que $A \in \mathbb{S}^n=U \Lambda U^T$. Alors




$$
x^T A x=x^T U \Lambda U^T x=y^T \Lambda y=\sideset{}{^n_{i=1}}\sum \lambda_i y_i^2
$$




o√π $y=U^T x$ (et puisque $U$ est de rang complet, tout vecteur $y \in \mathbb{R}^n$ peut √™tre repr√©sent√© sous cette forme). Comme $y_i^2$ est toujours positif, le signe de cette expression d√©pend enti√®rement des $\lambda_i$. Si tous les $\lambda_i>0$, alors la matrice est d√©finie positive ; si tous les $\lambda_i \geq 0$, elle est semi-d√©finie positive. De m√™me, si tous les $\lambda_i<0$ ou $\lambda_i \leq 0$, alors $A$ est d√©finie n√©gative ou semi-d√©finie n√©gative respectivement. Enfin, si $A$ a des valeurs propres √† la fois positives et n√©gatives, il est ind√©fini.

Une application o√π les valeurs propres et les vecteurs propres sont fr√©quemment utilis√©s est la maximisation d'une fonction d'une matrice. En particulier, pour une matrice $A \in \mathbb{S}^n$, on consid√®re le probl√®me de maximisation suivant,




$$
\max _{x \in \mathbb{R}^n} x^T A x \quad \text { subject to }\|x\|_2^2=1
$$




c'est-√†-dire que nous voulons trouver le vecteur (de norme 1) qui maximise la forme quadratique. En supposant que les valeurs propres sont ordonn√©es comme suit : $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$, le $x$ optimal pour ce probl√®me d'optimisation est $x_1$, le vecteur propre correspondant √† $\lambda_1$. Dans ce cas, la valeur maximale de la forme quadratique est $\lambda_1$. De m√™me, la solution optimale du probl√®me de minimisation,




$$
\min _{x \in \mathbb{R}^n} x^T A x \quad \text { subject to }\|x\|_2^2=1
$$




est $x_n$, le vecteur propre correspondant √† $\lambda_n$, et la valeur minimale est $\lambda_n$. Ceci peut √™tre prouv√© en faisant appel √† la forme vecteur propre-valeur propre de $A$ et aux propri√©t√©s des matrices orthogonales. Cependant, dans la section suivante, nous verrons un moyen de le d√©montrer directement en utilisant le calcul matriciel.

<a name="C-4"/>

### [C.4 Calcul matriciel](#c-4) ###

[Retour TOC](#toc)

Alors que les sujets des sections pr√©c√©dentes sont g√©n√©ralement abord√©s dans un cours standard d'alg√®bre lin√©aire, un sujet qui ne semble pas √™tre abord√© tr√®s souvent (et que nous utiliserons abondamment) est l'extension du calcul aux vecteurs. Bien que le calcul que nous utilisons soit relativement trivial, la notation peut souvent faire para√Ætre les choses beaucoup plus difficiles qu'elles ne le sont. Dans cette section, nous pr√©sentons quelques d√©finitions de base du calcul matriciel et fournissons quelques exemples.

<a name="C-4-1"/>

#### [C.4.1 Le gradient](#c-4-1) ####

[Retour TOC](#toc)

Supposons que $f : \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ est une fonction qui prend en entr√©e une matrice $A$ de taille $m \times n$ et retourne une valeur r√©elle. Alors le gradient de $f$ (par rapport √† $A \in \mathbb{R}^{m \times n}$ ) est la matrice des d√©riv√©es partielles, d√©finie comme suit :




$$
\nabla_A f(A) \in \mathbb{R}^{m \times n}=\left[\begin{array}{cccc}
\frac{\partial f(A)}{\partial A_1} & \frac{\partial f(A)}{\partial A_{13}} & \cdots & \frac{\partial f(A)}{\partial A_1} \\
\frac{\partial f(A)}{\partial A_{21}} & \frac{\partial f(A)}{\partial A_{22}} & \cdots & \frac{\partial f(A)}{\partial A_{2 n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f(A)}{\partial A_{m 1}} & \frac{\partial f(A)}{\partial A_{m 2}} & \cdots & \frac{\partial f(A)}{\partial A_{m n}}
\end{array}\right]
$$




c'est-√†-dire, une matrice de $m \times n$ avec




$$
\left(\nabla_A f(A)\right)_{i j}=\frac{\partial f(A)}{\partial A_{i j}} .
$$




Notez que la taille de $\nabla_A f(A)$ est toujours la m√™me que la taille de $A$. Donc, si, en particulier, $A$ est juste un vecteur $x \in \mathbb{R}^n$,




$$
\nabla_x f(x)=\left[\begin{array}{c}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_n}
\end{array}\right] .
$$




Il est tr√®s important de rappeler que le gradient d'une fonction n'est d√©fini que si la fonction est √† valeur r√©elle, c'est-√†-dire si elle renvoie une valeur scalaire. On ne peut pas, par exemple, prendre le gradient de $A x, A \in \mathbb{R}^{n \times n}$ par rapport √† $x$, puisque cette quantit√© est √† valeur vectorielle.

Il d√©coule directement des propri√©t√©s √©quivalentes des d√©riv√©es partielles que :

- $\nabla_x(f(x)+g(x))=\nabla_x f(x)+\nabla_x g(x)$.

- Pour $t \in \mathbb{R}, \nabla_x(t f(x))=t \nabla_x f(x)$.

Il est un peu plus difficile de d√©terminer l'expression correcte de $\nabla_x f(A x), A \in \mathbb{R}^{n \times n}$, mais c'est √©galement faisable (en fait, vous devrez r√©soudre ce probl√®me pour un devoir √† la maison).

<a name="C-4-2"/>

#### [C.4.2 Le hessien](#c-4-2) ####

[Retour TOC](#toc)

Supposons que $f : \mathbb{R}^n \rightarrow \mathbb{R}$ est une fonction qui prend un vecteur dans $\mathbb{R}^n$ et retourne un nombre r√©el. Alors la matrice hessienne par rapport √† $x$, √©crite $\nabla_x^2 f(x)$ ou simplement $H$ est la matrice $n \times n$ des d√©riv√©es partielles,




$$
\nabla_x^2 f(x) \in \mathbb{R}^{n \times n}=\left[\begin{array}{cccc}
\frac{\partial^2 f(x)}{\partial x_1^1} & \frac{\partial^2 f(x)}{\partial \partial_1 \theta x_2} & \cdots & \frac{\partial^2 f(x)}{\partial x_1 \partial_{x_n} x_n} \\
\frac{\partial^2 f(x)}{\partial x_x \partial \theta_1} & \frac{\partial^2 f(x)}{\partial x_2^2} & \cdots & \frac{\partial^2 f(x)}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f(x)}{\partial x_x \theta_1} & \frac{\partial^2 f(x)}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f(x)}{\partial x_n^2}
\end{array}\right] .
$$


En d'autres termes, $\nabla_x^2 f(x) \in \mathbb{R}^{n \times n}$, avec




$$
\left(\nabla_x^2 f(x)\right)_{i j}=\frac{\partial^2 f(x)}{\partial x_i \partial x_j} .
$$




Notez que le hessien est toujours sym√©trique, puisque




$$
\frac{\partial^2 f(x)}{\partial x_i \partial x_j}=\frac{\partial^2 f(x)}{\partial x_j \partial x_i}
$$




Tout comme le gradient, le hessien n'est d√©fini que lorsque $f(x)$ est √† valeur r√©elle.

Il est naturel de consid√©rer le gradient comme l'analogue de la d√©riv√©e premi√®re pour les fonctions de vecteurs, et le hessien comme l'analogue de la d√©riv√©e seconde (et les symboles que nous utilisons sugg√®rent √©galement cette relation). Cette intuition est g√©n√©ralement correcte, mais il y a quelques mises en garde √† garder √† l'esprit.

Premi√®rement, pour les fonctions √† valeur r√©elle d'une variable $f : \mathbb{R} \rightarrow \mathbb{R}$, il est une d√©finition de base que la d√©riv√©e seconde est la d√©riv√©e de la d√©riv√©e premi√®re, c'est-√†-dire,




$$
\frac{\partial^2 f(x)}{\partial x^2}=\frac{\partial}{\partial x} \frac{\partial}{\partial x} f(x) .
$$




Cependant, pour les fonctions d'un vecteur, le gradient de la fonction est un vecteur, et nous ne pouvons pas prendre le gradient d'un vecteur - c'est-√†-dire,




$$
\nabla_x \nabla_x f(x)=\nabla_x\left[\begin{array}{c}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_1}
\end{array}\right]
$$


et cette expression n'est pas d√©finie. Par cons√©quent, il n'est pas vrai que le hessien est le gradient du gradient. Cependant, c'est presque vrai, dans le sens suivant : si nous regardons la $i^{√®me}$ entr√©e du gradient $\left(\nabla_x f(x)\right)_i=\partial f(x) / \partial x_i$, et prenons le gradient par rapport √† $x$ nous obtenons




$$
\nabla_x \frac{\partial f(x)}{\partial x_i}=\left[\begin{array}{c}
\frac{\partial^2 f(x)}{\partial x_i \partial x_1} \\
\frac{\partial^2 f(x)}{\partial x_i \partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_i \partial x_n}
\end{array}\right]
$$




qui est la $i^{√®me}$ colonne (ou ligne) du Hessien. Par cons√©quent,




$$
\nabla_x^2 f(x)=\left[\begin{array}{llll}
\nabla_x\left(\nabla_x f(x)\right)_1 & \nabla_x\left(\nabla_x f(x)\right)_2 & \cdots & \nabla_x\left(\nabla_x f(x)\right)_n
\end{array}\right] .
$$




Si cela ne nous d√©range pas d'√™tre un peu n√©gligents, nous pouvons dire que (essentiellement) $\nabla_x^2 f(x)=\nabla_x\left(\nabla_x f(x)\right)^T$, tant que nous comprenons que cela signifie r√©ellement prendre le gradient de chaque entr√©e de $\left(\nabla_x f(x)\right)^T$, et non le gradient du vecteur entier.

Enfin, notez que bien que nous puissions prendre le gradient par rapport √† une matrice $A \in \mathbb{R}^n$, pour les besoins de ce cours, nous n'envisagerons de prendre le hessien que par rapport √† un vecteur $x \in \mathbb{R}^n$. Ceci est simplement une question de commodit√© (et le fait qu'aucun des calculs que nous faisons ne n√©cessite de trouver le hessien par rapport √† une matrice), puisque le hessien par rapport √† une matrice devrait repr√©senter toutes les d√©riv√©es partielles $\partial^2 f(A) /\left(\partial A_{i j} \partial A_{k \ell}\right)$, et il est plut√¥t encombrant de repr√©senter ceci sous forme de matrice.

<a name="C-4-3"/>

#### [C.4.3 Gradients et hessian des fonctions quadratiques et lin√©aires](#c-4-3) ####

[Retour TOC](#toc)

Essayons maintenant de d√©terminer les matrices du gradient et de la hessienne de quelques fonctions simples. Il convient de noter que tous les gradients donn√©s ici sont des cas particuliers des gradients donn√©s dans les notes de cours de CS229.

Pour $x \in \mathbb{R}^n$, soit $f(x)=b^T x$ pour un vecteur connu $b \in \mathbb{R}^n$. Alors




$$
f(x)=\sideset{}{^n_{i=1}}\sum b_i x_i
$$


donc




$$
\frac{\partial f(x)}{\partial x_k}=\frac{\partial}{\partial x_k} \sideset{}{^n_{i=1}}\sum b_i x_i=b_k .
$$




A partir de l√†, nous pouvons facilement voir que $\nabla_x b^T x=b$. Ceci doit √™tre compar√© √† la situation analogue dans le calcul √† une variable, o√π $\partial /(\partial x) a x=a$.

Consid√©rons maintenant la fonction quadratique $f(x)=x^T A x$ pour $A \in \mathbb{S}^n$. Rappelez-vous que




$$
f(x)=\sideset{}{^n_{i=1}}\sum \sideset{}{^n_{j=1}}\sum A_{i j} x_i x_j
$$


donc




$$
\frac{\partial f(x)}{\partial x_k}=\frac{\partial}{\partial x_k} \sideset{}{^n_{i=1}}\sum \sideset{}{^n_{j=1}}\sum A_{i j} x_i x_j=\sideset{}{^n_{i=1}}\sum A_{i k} x_i+\sideset{}{^n_{j=1}}\sum A_{k j} x_j=2 \sideset{}{^n_{i=1}}\sum A_{k i} x_i
$$




o√π la derni√®re √©galit√© suit puisque $A$ est sym√©trique (ce que nous pouvons supposer sans risque, puisqu'il appara√Æt sous une forme quadratique). Notez que la $k^{√®me}$ entr√©e de $\nabla_x f(x)$ est juste le produit interne de la $k^{√®me}$ ligne de $A$ et de $x$. Par cons√©quent, $\nabla_x x^T A x=2 A x$. Encore une fois, cela devrait vous rappeler le fait analogue dans le calcul √† une variable, √† savoir que $\partial /(\partial x) a x^2=2 a x$.

Enfin, examinons le hessien de la fonction quadratique $f(x)=x^T A x$ (il devrait √™tre √©vident que le hessien d'une fonction lin√©aire $b^T x$ est nul). C'est encore plus facile que de d√©terminer le gradient de la fonction, puisque


$$
\frac{\partial^2 f(x)}{\partial x_k \partial x_{\ell}}=\frac{\partial^2}{\partial x_k \partial x_{\ell}} \sideset{}{^n_{i=1}}\sum \sideset{}{^n_{j=1}}\sum A_{i j} x_i x_j=A_{k \ell}+A_{\ell k}=2 A_{k \ell} .
$$


Par cons√©quent, il devrait √™tre clair que $\nabla_x^2 x^T A x=2 A$, ce qui devrait √™tre tout √† fait attendu (et √† nouveau analogue au fait √† une seule variable que $\left.\partial^2 /\left(\partial x^2\right) a x^2=2 a\right)$.

Pour r√©sumer,

- $\nabla_x b^T x=b$

- $\nabla_x x^T A x=2 A x$ (if $A$ symmetric)

- $\nabla_x^2 x^T A x=2 A$ (if $A$ symmetric)

<a name="C-4-4"/>

#### [C.4.4 Les moindres carr√©s](#c-4-4) ####

[Retour TOC](#toc)

Appliquons les √©quations que nous avons obtenues dans la section pr√©c√©dente pour d√©river les √©quations des moindres carr√©s. Supposons que l'on nous donne des matrices $A \in \mathbb{R}^{m \times n}$ (pour simplifier, nous supposons que $A$ est de rang complet) et un vecteur $b \in \mathbb{R}^m$ tel que $b \notin \mathcal{R}(A)$. Dans cette situation, nous ne serons pas capables de trouver un vecteur $x \in \mathbb{R}^n$, tel que $A x = b$, donc √† la place nous voulons trouver un vecteur $x$ tel que $A x$ soit aussi proche que possible de $b$, tel que mesur√© par le carr√© de la norme euclidienne $\|A x-b\|_2^2$.

En utilisant le fait que $\|x\|_2^2=x^T x$, nous avons




$$
\begin{aligned}
\|A x-b\|_2^2 & =(A x-b)^T(A x-b) \\
& =x^T A^T A x-2 b^T A x+b^T b
\end{aligned}
$$




En prenant le gradient par rapport √† $x$ nous avons, et en utilisant les propri√©t√©s que nous avons d√©riv√©es dans la section pr√©c√©dente




$$
\begin{aligned}
\nabla_x\left(x^T A^T A x-2 b^T A x+b^T b\right) & =\nabla_x x^T A^T A x-\nabla_x 2 b^T A x+\nabla_x b^T b \\
& =2 A^T A x-2 A^T b
\end{aligned}
$$


En mettant cette derni√®re expression √©gale √† z√©ro et en r√©solvant pour $x$ on obtient les √©quations normales




$$
x=\left(A^T A\right)^{-1} A^T b
$$




ce qui est identique √† ce que nous avons d√©riv√© en classe.

<a name="C-4-5"/>

#### [C.4.5 Gradients du d√©terminant](#c-4-5) ####

[Retour TOC](#toc)

Consid√©rons maintenant une situation o√π nous trouvons le gradient d'une fonction par rapport √† une matrice, √† savoir pour $A \in \mathbb{R}^{n \times n}$, nous voulons trouver $\nabla_A|A|$. Rappelons de notre discussion sur les d√©terminants que




$$
|A|=\sideset{}{^n_{i=1}}\sum(-1)^{i+j} A_{i j}|A_{\backslash i, \backslash j}| \quad \text { (for any } j \in 1, \ldots, n \text { ) }
$$




donc




$$
\frac{\partial}{\partial A_{k \ell}}|A|=\frac{\partial}{\partial A_{k \ell}} \sideset{}{^n_{i=1}}\sum(-1)^{i+j} A_{i j}|A_{\backslash i, \backslash j}|=(-1)^{k+\ell}|A_{\backslash k, \backslash \ell}|=({adj}(A))_{\ell k} .
$$




Il s'ensuit imm√©diatement des propri√©t√©s de l'adjoint que




$$
\nabla_A|A|=({adj}(A))^T=|A| A^{-T}
$$




Consid√©rons maintenant la fonction $f : \mathbb{S}_{++}^n \rightarrow \mathbb{R}, f(A)=\log |A|$. Notez que nous devons restreindre le domaine de $f$ aux matrices d√©finies positives, car cela garantit que $|A|>0$, de sorte que le $\log$ de $|A|$ est un nombre r√©el. Dans ce cas, nous pouvons utiliser la r√®gle de la cha√Æne (rien d'extraordinaire, juste la r√®gle de la cha√Æne ordinaire du calcul √† une variable) pour voir que




$$
\frac{\partial \log |A|}{\partial A_{i j}}=\frac{\partial \log |A|}{\partial|A|} \frac{\partial|A|}{\partial A_{i j}}=\frac{1}{|A|} \frac{\partial|A|}{\partial A_{i j}} .
$$




Il est donc √©vident que




$$
\nabla_A \log |A|=\frac{1}{|A|} \nabla_A|A|=A^{-1},
$$




o√π nous pouvons laisser tomber la transposition dans la derni√®re expression car $A$ est sym√©trique. Notez la similitude avec le cas √† valeur unique, o√π $\partial /(\partial x) \log x=1 / x$.

<a name="C-4-6"/>

#### [C.4.6 Valeurs propres en tant qu'optimisation](#c-4-6) ####

[Retour TOC](#toc)

Enfin, nous utilisons le calcul matriciel pour r√©soudre un probl√®me d'optimisation d'une mani√®re qui m√®ne directement √† l'analyse des valeurs propres/vecteurs propres. Consid√©rons le probl√®me d'optimisation suivant, soumis √† des contraintes d'√©galit√© :




$$
\max _{x \in \mathbb{R}^n} x^T A x \quad \text { subject to }\|x\|_2^2=1
$$




pour une matrice sym√©trique $A \in \mathbb{S}^n$. Une fa√ßon standard de r√©soudre les probl√®mes d'optimisation avec des contraintes d'√©galit√© est de former le Lagrangien, une fonction objectif qui inclut les contraintes d'√©galit√©. ${ }^2$ Le lagrangien dans ce cas peut √™tre donn√© par




$$
\mathcal{L}(x, \lambda)=x^T A x-\lambda x^T x
$$




o√π $\lambda$ est appel√© le multiplicateur de Lagrange associ√© √† la contrainte d'√©galit√©. On peut √©tablir que pour que $x^*$ soit un point optimal du probl√®me, le gradient du Lagrangien doit √™tre nul √† $x^*$ (ce n'est pas la seule condition, mais elle est requise). C'est-√†-dire ,




$$
\nabla_x \mathcal{L}(x, \lambda)=\nabla_x\left(x^T A x-\lambda x^T x\right)=2 A^T x-2 \lambda x=0 .
$$




Remarquez que c'est juste l'√©quation lin√©aire $A x=\lambda x$. Cela montre que les seuls points qui peuvent √©ventuellement maximiser (ou minimiser) $x^T A x$ en supposant que $x^T x=1$ sont les vecteurs propres de $A$.
